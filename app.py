import streamlit as st
import pandas as pd
import polars as pl  # –ë–ò–ë–õ–ò–û–¢–ï–ö–ê –î–õ–Ø –°–ö–û–†–û–°–¢–ò
import json
import re
import os
import random
import numpy as np
import requests
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from wordcloud import WordCloud
from collections import Counter, defaultdict
from datetime import datetime, timedelta
import emoji 
from PIL import Image, UnidentifiedImageError
from streamlit_lottie import st_lottie

# –ü–û–ü–´–¢–ö–ê –ò–ú–ü–û–†–¢–ê –§–†–ê–ì–ú–ï–ù–¢–û–í (–î–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –í–∏–∫—Ç–æ—Ä–∏–Ω—ã)
try:
    from streamlit import fragment
except ImportError:
    try:
        from streamlit import experimental_fragment as fragment
    except ImportError:
        def fragment(func):
            return func
import time
# === –ù–ê–°–¢–†–û–ô–ö–ò –î–ê–¢–´ ===
# –£–∫–∞–∂–∏—Ç–µ –∑–¥–µ—Å—å —Ç—É –∂–µ –¥–∞—Ç—É, –æ—Ç –∫–æ—Ç–æ—Ä–æ–π –∏–¥–µ—Ç –æ—Ç—Å—á–µ—Ç –≤ —Ö–µ–¥–µ—Ä–µ
# –§–æ—Ä–º–∞—Ç: –ì–æ–¥, –ú–µ—Å—è—Ü, –î–µ–Ω—å
REL_START_DATE = pd.Timestamp(datetime(2025, 9, 14))
# --- DEBUG & PROFILING TOOL (–û–¢–õ–ê–î–ö–ê) ---
# –≠—Ç–æ—Ç –∫–ª–∞—Å—Å –ø–æ–º–æ–∂–µ—Ç –Ω–∞–º –ø–æ–Ω—è—Ç—å, –Ω–∞ —á–µ–º –∏–º–µ–Ω–Ω–æ —Ç–æ—Ä–º–æ–∑–∏—Ç –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
class Profiler:
    def __init__(self):
        self.log = []
        self.start_global = time.time()
        self.last_check = self.start_global

    def checkpoint(self, label):
        now = time.time()
        duration = now - self.last_check
        self.log.append(f"‚è± {label}: {duration:.4f} —Å–µ–∫")
        self.last_check = now

    def finish(self):
        total = time.time() - self.start_global
        self.log.append(f"üèÅ –í–°–ï–ì–û: {total:.4f} —Å–µ–∫")
        # –í—ã–≤–æ–¥–∏–º –≤ —Å–∞–π–¥–±–∞—Ä (–º–æ–∂–Ω–æ —Å–≤–µ—Ä–Ω—É—Ç—å)
        with st.sidebar.expander("üõ† –û—Ç–ª–∞–¥–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏", expanded=False):
            st.code("\n".join(self.log), language="text")

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–æ—Ñ–∞–π–ª–µ—Ä –≤ –Ω–∞—á–∞–ª–µ —Å–∫—Ä–∏–ø—Ç–∞
profiler = Profiler()
# ---------------- –ù–ê–°–¢–†–û–ô–ö–ò ----------------
st.set_page_config(page_title="100 –¥–Ω–µ–π –≤–º–µ—Å—Ç–µ", page_icon="üéÄ", layout="wide")
CACHE_FILE = "optimized_chat.parquet"
# ---------------- –°–¢–ò–õ–¨ (–ù–û–í–´–ô –î–ò–ó–ê–ô–ù) ----------------
st.markdown("""
<style>
    @import url('https://fonts.googleapis.com/css2?family=Nunito:wght@400;600;800&family=Pacifico&display=swap');

    /* –ì–ª–æ–±–∞–ª—å–Ω—ã–π —Ñ–æ–Ω */
    .stApp { 
        background: linear-gradient(135deg, #fff0f5 0%, #fff5ee 100%);
        font-family: 'Nunito', sans-serif;
    }
    
    h1, h2, h3 { color: #FF69B4 !important; font-family: 'Nunito', sans-serif; font-weight: 800; }
    h1 { font-family: 'Pacifico', cursive; letter-spacing: 2px; }

    /* --- –ù–û–í–´–ô HERO HEADER --- */
    .hero-container {
        background: linear-gradient(120deg, #ff9a9e 0%, #fecfef 100%);
        border-radius: 25px;
        padding: 40px 20px;
        text-align: center;
        color: white;
        box-shadow: 0 15px 30px rgba(255, 105, 180, 0.3);
        margin-bottom: 30px;
        position: relative;
        overflow: hidden;
    }
    .hero-title { font-family: 'Pacifico', cursive; font-size: 3em; margin: 0; text-shadow: 2px 2px 4px rgba(0,0,0,0.1); }
    .hero-days { font-size: 5em; font-weight: 800; line-height: 1; margin: 10px 0; }
    .hero-subtitle { font-size: 1.2em; font-weight: 600; opacity: 0.9; }
    .heart-beat { animation: heartbeat 1.5s infinite; display: inline-block; }
    
    .winner-box { background: rgba(255,255,255,0.6); backdrop-filter: blur(10px); padding: 20px; border-radius: 20px; border: 2px solid rgba(255, 204, 213, 0.5); text-align: center; margin-bottom: 15px; box-shadow: 0 8px 32px 0 rgba(31,38,135,0.07); transition: transform 0.3s ease; }
    .winner-box:hover { transform: translateY(-5px); border: 2px solid rgba(255, 204, 213, 1); }
    .winner-name { color: #FF69B4; font-size: 20px; font-weight: 800; margin: 5px 0; }
                    
    @keyframes heartbeat {
        0% { transform: scale(1); }
        50% { transform: scale(1.2); }
        100% { transform: scale(1); }
    }

    /* --- –ù–û–í–´–ô TIMELINE (–ò–°–¢–û–†–ò–Ø) --- */
    .timeline-container {
        position: relative;
        padding: 20px 0;
    }
    .timeline-item {
        position: relative;
        padding-left: 40px;
        margin-bottom: 30px;
        border-left: 3px solid #ffccd5;
    }
    .timeline-dot {
        position: absolute;
        left: -9px;
        top: 0;
        width: 15px;
        height: 15px;
        border-radius: 50%;
        background: #FF69B4;
        border: 3px solid white;
        box-shadow: 0 0 0 2px #FF69B4;
    }
    .timeline-date {
        font-size: 0.85em;
        color: #FF69B4;
        font-weight: 700;
        text-transform: uppercase;
        margin-bottom: 5px;
        display: block;
    }
    .timeline-card {
        background: white;
        border-radius: 15px;
        padding: 15px;
        box-shadow: 0 5px 15px rgba(0,0,0,0.05);
        border: 1px solid #fff0f5;
        transition: transform 0.2s;
    }
    .timeline-card:hover { transform: translateX(5px); border-color: #FFB6C1; }
    .timeline-icon { font-size: 1.5em; margin-right: 10px; float: left; }
    .timeline-content { margin-left: 40px; }
    .timeline-title { font-weight: 800; color: #444; font-size: 1.1em; margin-bottom: 5px; }
    .timeline-text { font-size: 0.95em; color: #666; font-style: italic; }
    .timeline-author { font-size: 0.8em; color: #aaa; margin-top: 5px; text-align: right; }

    /* –û–°–¢–ê–õ–¨–ù–´–ï –°–¢–ò–õ–ò (–°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ä—ã–µ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏) */
        div[data-testid="stMetric"], .first-time-box, .prediction-box {
        background-color: white;
        border-radius: 20px;
        padding: 20px;
        box-shadow: 0 10px 25px rgba(255, 105, 180, 0.1);
        border: 1px solid #fff0f5;
        transition: transform 0.2s ease;
    }
    
    div[data-testid="stMetric"]:hover {
        transform: translateY(-5px);
        border-color: #FFB6C1;
    }
    div[data-testid="stVerticalBlockBorderWrapper"], .stVerticalBlockBorderWrapper {
        background-color: white !important; border: 1px solid #ffeef2 !important; 
        border-radius: 20px !important; padding: 20px !important; margin-bottom: 20px !important;
    }
    .stTabs [data-baseweb="tab-list"] {
        gap: 15px; background-color: rgba(255, 255, 255, 0.6); padding: 15px;
        border-radius: 25px; flex-wrap: wrap; box-shadow: 0 4px 15px rgba(0,0,0,0.03);
    }
    .stTabs [data-baseweb="tab"] {
        background-color: white; border-radius: 15px; padding: 10px 25px;
        border: 1px solid #ffe4e1; font-weight: 600; color: #888;
    }
    .stTabs [aria-selected="true"] {
        background: linear-gradient(45deg, #FF69B4, #FFB6C1) !important; color: white !important;
        border: none; box-shadow: 0 4px 12px rgba(255, 105, 180, 0.4);
    }
    .sticker-context-box {
        background-color: #f8f9fa; border-left: 4px solid #FF69B4; padding: 8px 12px;
        margin-top: 8px; border-radius: 0 8px 8px 0; font-size: 14px; color: #333; font-weight: 600;
    }
    .quiz-text{ text-align: center;
                font-size: 20px;
                font-weight: 800;
                margin: 5px 0;
                background-color: white;
                border-radius: 10px;
                width: 100%;
                height: 100px;
                border: 2px solid rgba(255, 204, 213, 0.5);
                transition: ease 0.5s;
                padding:10px;
                align-content: center;
               }
    .quiz-text:hover{ text-align: center;
                font-size: 20px;
                font-weight: 850;
                margin: 5px 0;
                background-color: white;
                border-radius: 10px;
                width: 100%;
                height: 100px;
                border: 2px solid rgba(255, 204, 213, 1);
                box-shadow: rgba(0,0,0,0.1) 3px 4px;
                align-content: center;
               }  
    .stProgress > div > div > div > div {
                background: linear-gradient(120deg, #ff9a9e 0%, #fecfef 100%);
            }   
    .st-emotion-cache-17qp3xt {
                width: calc(100% - 1rem);
                flex: 1 1 calc(100% - 1rem);
            }
    .st-emotion-cache-1ne20ew{
            -moz-box-pack: start;
            border-radius: 0.5rem;
            overflow: visible;
            display: flex;
            gap: 1rem;
            width: 100%;
            max-width: 100%;
            height: auto;
            min-width: 1rem;
            flex-flow: column;
            flex: 1 1 0%;
            -moz-box-align: start;
            align-items: start;
            justify-content: start;
            border: 2px solid rgba(255, 204, 213, 0.3);
            background-color: white;
            padding: calc(2rem);
            transition:ease 0.5s;
            }
    .st-emotion-cache-1ne20ew:hover{
            transform: translateY(-5px); border: 2px solid rgba(255, 204, 213, 1);
            }
    .st-f0 {
            background-color: rgba(49, 51, 63, 0);
            }
    input[type=text]{background-color:white;}
    input[type=text]::placeholder {
            color: rgba(49, 51, 63, 0.6);
            }
    div[data-baseweb="tab-highlight"],div[data-baseweb="tab-border"]{visibility:hidden;}
</style>
""", unsafe_allow_html=True)

# ---------------- –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò ----------------
STOP_WORDS = {'–∏','–≤','–≤–æ','–Ω–µ','—á—Ç–æ','–æ–Ω','–Ω–∞','—è','—Å','—Å–æ','–∫–∞–∫','–∞','—Ç–æ','–≤—Å–µ','–æ–Ω–∞','—Ç–∞–∫','–µ–≥–æ','–Ω–æ','–¥–∞','—Ç—ã','–∫','—É','–∂–µ','–≤—ã','–∑–∞','–±—ã','–ø–æ','–µ–µ','–º–Ω–µ','–≤–æ—Ç','–æ—Ç','–º–µ–Ω—è','–µ—â–µ','–Ω–µ—Ç','–æ','–∏–∑','–µ–º—É','–∫–æ–≥–¥–∞','–Ω—É','–∏–ª–∏','–º—ã','—Ç–µ–±—è','–∏—Ö','–±—ã–ª–∞','—á—Ç–æ–±','–±–µ–∑','–±—É–¥—Ç–æ','–±—É–¥–µ—Ç','—Ç–æ–≥–¥–∞','–∫—Ç–æ','—ç—Ç–æ','–ø—Ä–æ—Å—Ç–æ','–æ—á–µ–Ω—å','–ª–∞–¥–Ω–æ','—â–∞—Å','–ø–æ—á–µ–º—É','—á–µ—Ä–µ–∑','–≤—Å—ë','–µ—â—ë','–ø—Ä–æ','—Ç–æ–ª—å–∫–æ','–±—ã–ª–æ','—Ç–µ–ø–µ—Ä—å','–¥–∞–∂–µ','–≤–¥—Ä—É–≥','–ª–∏','–µ—Å–ª–∏','—É–∂–µ','–Ω–∏','–±—ã—Ç—å','–±—ã–ª','–Ω–µ–≥–æ','–¥–æ','–≤–∞—Å','–Ω–∏–±—É–¥—å','–æ–ø—è—Ç—å','—É–∂','–≤–∞–º','–≤–µ–¥—å','—Ç–∞–º','–ø–æ—Ç–æ–º','—Å–µ–±—è','–Ω–∏—á–µ–≥–æ','–µ–π','–º–æ–∂–µ—Ç','–æ–Ω–∏','—Ç—É—Ç','–≥–¥–µ','–µ—Å—Ç—å','–Ω–∞–¥–æ','–Ω–µ–π','–¥–ª—è','—á–µ–º','—Å–∞–º','—á–µ–≥–æ','—Ä–∞–∑','—Ç–æ–∂–µ','—Å–µ–±–µ','–ø–æ–¥','–∂','—ç—Ç–æ—Ç','—Ç–æ–≥–æ','–ø–æ—Ç–æ–º—É','—ç—Ç–æ–≥–æ','–∫–∞–∫–æ–π','—Å–æ–≤—Å–µ–º','–Ω–∏–º','–∑–¥–µ—Å—å','—ç—Ç–æ–º','–æ–¥–∏–Ω','–ø–æ—á—Ç–∏','–º–æ–π','—Ç–µ–º','—á—Ç–æ–±—ã','–≤–æ–æ–±—â–µ','—Ç–∏–ø–æ','–∫–∞–ø–µ—Ü','–Ω–∞–≤–µ—Ä–Ω–æ–µ','–±–ª–∏–Ω','–∞—Ö–∞—Ö–∞','–ø—Ö–ø—Ö','—Ö–∞—Ö–∞','–∫–∞–∂–µ—Ç—Å—è','—Ç–∞–∫–æ–π','–∫–æ—Ç–æ—Ä—ã–π','—Ö–æ—Ç—è','–±—É–¥—É','—Ç–µ–±–µ','–ø—Ä–∏–≤–µ—Ç','–∑–Ω–∞—é','–ø—Ö—Ö–ø—Ö–ø—Ö–ø','–≤—Ö–≤—Ö–∞—Ö–∞—Ö','–≤—Ö–≤—Ö–≤—Ö—Ö–≤'}


def clean_text(text):
    if text is None: return ""
    # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –±—É–∫–≤—ã –∏ –ø—Ä–æ–±–µ–ª—ã
    text = re.sub(r'[^–∞-—è—ë\s]', '', str(text).lower())
    return " ".join(w for w in text.split() if w not in STOP_WORDS and len(w) > 2)

def clean_text_for_prediction(text):
    text = re.sub(r'[^–∞-—è—ë\s]', '', str(text).lower())
    return text.split()
@st.cache_data
def get_ngrams(text_series, n=2, top_k=10):
    all_text = " ".join(text_series.dropna().apply(clean_text))
    words = all_text.split()
    if len(words) < n: return []
    ngrams = zip(*[words[i:] for i in range(n)])
    return Counter([" ".join(ngram) for ngram in ngrams]).most_common(top_k)

def extract_emojis(text):
    return [c for c in text if c in emoji.EMOJI_DATA]


def format_time(minutes):
    if pd.isna(minutes) or minutes == 0: return "0 —Å–µ–∫"
    seconds = int(minutes * 60)
    mins = seconds // 60
    secs = seconds % 60
    parts = []
    if mins > 0: parts.append(f"{mins} –º–∏–Ω")
    if secs > 0 or mins == 0: parts.append(f"{secs} —Å–µ–∫")
    return " ".join(parts)

@st.cache_data
def build_markov_model(text_series):
    model = defaultdict(list)
    for text in text_series:
        words = clean_text_for_prediction(text)
        for i in range(len(words) - 1):
            model[words[i]].append(words[i+1])
    return model

def predict_phrase(model, seed_word, length=7):
    current_word = seed_word.lower().strip()
    sentence = [current_word]
    for _ in range(length):
        if current_word in model:
            next_options = model[current_word]
            word_counts = Counter(next_options)
            words, counts = zip(*word_counts.items())
            current_word = random.choices(words, weights=counts, k=1)[0]
            sentence.append(current_word)
        else:
            break
    return " ".join(sentence).capitalize()

@st.cache_data
def parse_discord_data(filepath):
    """–ü–∞—Ä—Å–∏–Ω–≥ JSON –æ—Ç DiscordChatExporter (Fix: –ò—Å–ø—Ä–∞–≤–ª–µ–Ω —Ñ–æ—Ä–º–∞—Ç –≤—Ä–µ–º–µ–Ω–∏)"""
    if not os.path.exists(filepath):
        return pd.DataFrame()
    
    try:
        with open(filepath, encoding="utf-8") as f:
            data = json.load(f)
        
        msgs = data.get("messages", [])
        
        parsed = []
        for m in msgs:
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
            if m.get("type") not in ["Default", "Reply"]:
                continue
                
            parsed.append({
                "date": m.get("timestamp"),
                "from": m.get("author", {}).get("name", "Unknown"),
                "text": m.get("content", ""),
                "file": m.get("attachments", [{}])[0].get("url") if m.get("attachments") else None,
                "media_type": "photo" if m.get("attachments") else None
            })
            
        df = pd.DataFrame(parsed)
        if not df.empty:
            # !!! –ò–ó–ú–ï–ù–ï–ù–ò–ï –ó–î–ï–°–¨ !!!
            # 1. format='mixed' –ø–æ–∑–≤–æ–ª—è–µ—Ç Pandas —Å–∞–º–æ–º—É —Ä–∞–∑–æ–±—Ä–∞—Ç—å—Å—è, –≥–¥–µ –¥–µ–Ω—å, –∞ –≥–¥–µ –º–µ—Å—è—Ü, –¥–∞–∂–µ –µ—Å–ª–∏ —Ñ–æ—Ä–º–∞—Ç—ã —Å–∫–∞—á—É—Ç.
            # 2. utc=True –ø—Ä–∏–≤–æ–¥–∏—Ç —Ç–∞–π–º–∑–æ–Ω—É –∫ –Ω—É–ª—é (—á—Ç–æ–±—ã +02:00 –∏ +03:00 —Å—Ç–∞–ª–∏ –æ–¥–Ω–∏–º –≤—Ä–µ–º–µ–Ω–µ–º).
            # 3. .dt.tz_localize(None) —É–±–∏—Ä–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–∞–π–º–∑–æ–Ω–µ —Å–æ–≤—Å–µ–º, —á—Ç–æ–±—ã –º–æ–∂–Ω–æ –±—ã–ª–æ —Å–∫–ª–µ–∏—Ç—å —Å –¥–∞–Ω–Ω—ã–º–∏ Telegram.
            df["date"] = pd.to_datetime(df["date"], format='mixed', utc=True).dt.tz_localize(None)
            
            df["text"] = df["text"].astype(str)
            
        return df
    except Exception as e:
        # –í—ã–≤–æ–¥–∏–º –¥–µ—Ç–∞–ª—å–Ω—É—é –æ—à–∏–±–∫—É, –µ—Å–ª–∏ —Å–Ω–æ–≤–∞ —É–ø–∞–¥–µ—Ç
        st.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞—Ç –≤ Discord: {e}")
        return pd.DataFrame()
@st.cache_data
def Create_word_Cloud():
    profiler.checkpoint("–ù–∞—á–∞–ª–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ–±–ª–∞–∫–∞")
    st.subheader("‚òÅÔ∏è –û–±–ª–∞–∫–æ –ª—é–±–≤–∏")
    all_words = " ".join(df["text"].apply(clean_text))
    
    if all_words:
        try:
            mask = np.array(Image.open("heart_mask.png"))
        except:
            mask = None

        wc = WordCloud(
            width=1000, height=800, 
            background_color="white", 
            colormap="Reds",
            mask=mask,
            contour_width=2, 
            contour_color='firebrick',
            font_path="arial.ttf" if os.path.exists("arial.ttf") else None
        ).generate(all_words)
        
        st.image(wc.to_array(), width='stretch')
        profiler.checkpoint("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–±–ª–∞–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
# ================== –§–ò–ù–ê–õ–¨–ù–´–ô –ë–õ–û–ö –ó–ê–ì–†–£–ó–ö–ò (–ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô) ==================

# ================== –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –ó–ê–ì–†–£–ó–ö–ê (–í–ï–†–ù–£–õ–ò file) ==================

def prepare_text_for_polars(val):
    if isinstance(val, str):
        return val
    elif isinstance(val, list):
        res = []
        for part in val:
            if isinstance(part, str):
                res.append(part)
            elif isinstance(part, dict) and "text" in part:
                res.append(str(part["text"]))
        return "".join(res)
    elif isinstance(val, dict):
        return val.get("text", "")
    elif val is None:
        return ""
    else:
        return str(val)

@st.cache_data(show_spinner="–û–±—Ä–∞–±–æ—Ç–∫–∞ –∞—Ä—Ö–∏–≤–∞... (–≤ –ø–µ—Ä–≤—ã–π —Ä–∞–∑ —ç—Ç–æ –∑–∞–π–º–µ—Ç –≤—Ä–µ–º—è)")
def load_data():
    # 1. –ü–†–û–í–ï–†–ö–ê –ö–≠–®–ê
    if os.path.exists(CACHE_FILE):
        try:
            return pl.read_parquet(CACHE_FILE).to_pandas()
        except Exception as e:
            st.warning(f"–ö—ç—à –ø–æ–≤—Ä–µ–∂–¥–µ–Ω, –ø–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º: {e}")

    # 2. –ó–ê–ì–†–£–ó–ö–ê –ò–ó JSON
    data_frames = []
    
    if os.path.exists("result.json"):
        try:
            with open("result.json", encoding="utf-8") as f:
                json_data = json.load(f)
            
            # --- –®–∞–≥ A: –ì—Ä—É–∑–∏–º –≤ Pandas ---
            df_pandas = pd.DataFrame(json_data["messages"])
            
            if "from" in df_pandas.columns:
                df_pandas = df_pandas.dropna(subset=["from"])
            
            # --- –®–∞–≥ B: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö ---
            # 1. –ß–∏—Å—Ç–∏–º —Ç–µ–∫—Å—Ç
            if "text" in df_pandas.columns:
                df_pandas["text"] = df_pandas["text"].apply(prepare_text_for_polars)
            
            # 2. –ß–∏—Å—Ç–∏–º –¥–∞—Ç—ã
            if "date" in df_pandas.columns:
                df_pandas["date"] = pd.to_datetime(df_pandas["date"], format="%Y-%m-%dT%H:%M:%S", errors='coerce')

            # 3. –ì–ê–†–ê–ù–¢–ò–†–£–ï–ú –ù–ê–õ–ò–ß–ò–ï –ö–û–õ–û–ù–û–ö (Fix KeyError: 'file')
            # –ï—Å–ª–∏ –∫–∞–∫–∏—Ö-—Ç–æ –∫–æ–ª–æ–Ω–æ–∫ –Ω–µ—Ç –≤ JSON, —Å–æ–∑–¥–∞–µ–º –∏—Ö –ø—É—Å—Ç—ã–º–∏, —á—Ç–æ–±—ã –∫–æ–¥ –Ω–µ –ø–∞–¥–∞–ª
            needed_cols = ["file", "photo", "media_type", "thumbnail"]
            for col in needed_cols:
                if col not in df_pandas.columns:
                    df_pandas[col] = None

            # 4. –í—ã–±–∏—Ä–∞–µ–º –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è (–¥–æ–±–∞–≤–∏–ª–∏ file, photo –∏ —Ç.–¥.)
            cols_to_keep = ["id", "type", "date", "from", "text", "file", "photo", "media_type"]
            # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ, —á—Ç–æ —Ä–µ–∞–ª—å–Ω–æ –µ—Å—Ç—å (–Ω–∞ —Å–ª—É—á–∞–π –µ—Å–ª–∏ JSON —Å–æ–≤—Å–µ–º —Å—Ç—Ä–∞–Ω–Ω—ã–π)
            final_cols = [c for c in cols_to_keep if c in df_pandas.columns]
            df_pandas = df_pandas[final_cols]

            # --- –®–∞–≥ C: –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ Polars ---
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤—Å–µ –æ–±—ä–µ–∫—Ç—ã –≤ —Å—Ç—Ä–æ–∫–∏, —á—Ç–æ–±—ã Polars –Ω–µ —Ä—É–≥–∞–ª—Å—è –Ω–∞ —Å–º–µ—à–∞–Ω–Ω—ã–µ —Ç–∏–ø—ã –≤ file/photo
            # (—Ç–∞–º –º–æ–∂–µ—Ç –±—ã—Ç—å null –∏–ª–∏ string)
            df_pl = pl.from_pandas(df_pandas)
            
            data_frames.append(df_pl)

        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è result.json: {e}")

    if os.path.exists("discord.json"):
        try:
             pass # –¢—É—Ç –∫–æ–¥ –¥–∏—Å–∫–æ—Ä–¥–∞
        except:
            pass

    if not data_frames:
        return None

    df_final = pl.concat(data_frames, how="diagonal")

    # 3. –ü–†–ï–î–í–´–ß–ò–°–õ–ï–ù–ò–Ø
    df_final = df_final.with_columns(
        pl.col("text").str.len_chars().fill_null(0).alias("len")
    )
    
    df_final = df_final.with_columns(
        ((pl.col("len") > 30) & 
         (pl.col("len") < 250) & 
         (~pl.col("text").str.contains("http"))).alias("is_quiz_candidate")
    )

    name_mapping = {
        "my princessüñ§": "–ü—Ä–∏–Ω—Ü–µ—Å—Å–∞", "kiss_freak": "–ü—Ä–∏–Ω—Ü–µ—Å—Å–∞",
        "tenfy_": "–ú–∏–ª—ã–π", "April": "–ú–∏–ª—ã–π"
    }
    # –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –∑–∞–º–µ–Ω–∞ –∏–º–µ–Ω
    if "from" in df_final.columns:
        df_final = df_final.with_columns(
            pl.col("from").replace(name_mapping, default=pl.col("from"))
        )

    df_final = df_final.sort("date")

    # 4. –°–û–•–†–ê–ù–ï–ù–ò–ï
    df_final.write_parquet(CACHE_FILE)
    
    return df_final.to_pandas()

# –û–ü–†–ï–î–ï–õ–Ø–ï–ú –ú–û–î–ê–õ–¨–ù–û–ï –û–ö–ù–û (DIALOG)
@st.dialog("–ü–æ–¥—Ä–æ–±–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞")
def show_winner_details(title, description, sorted_items, suffix):
    st.markdown(f"### {title}")
    st.info(description)
    st.markdown("---")
    st.markdown("#### üìä –†–µ–π—Ç–∏–Ω–≥ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤:")
    
    if sorted_items:
        winner_name = sorted_items[0][0]
        for name, score in sorted_items:
            if isinstance(score, float):
                val = format_time(score) if suffix == "–º–∏–Ω" else f"{score:.1f}"
            else:
                val = str(score)
            
            final_suffix = "" if suffix == "–º–∏–Ω" else suffix
            
            if name == winner_name:
                st.markdown(f"ü•á **{name}: {val} {final_suffix}**")
            else:
                st.markdown(f"üîπ {name}: {val} {final_suffix}")

# –§–£–ù–ö–¶–ò–Ø –ö–ê–†–¢–û–ß–ï–ö
def draw_winner_card(title, stats_dict, emoji_icon="üèÜ", suffix="", reverse=False, description=""):
    if not stats_dict: return
    
    sorted_items = sorted(stats_dict.items(), key=lambda item: item[1], reverse=not reverse)
    if not sorted_items: return
    
    winner, win_score = sorted_items[0]
    
    if isinstance(win_score, float):
        score_display = format_time(win_score) if suffix == "–º–∏–Ω" else f"{win_score:.1f}"
    else:
        score_display = str(win_score)
        
    final_suffix = "" if suffix == "–º–∏–Ω" else suffix

    # –†–∏—Å—É–µ–º –∫—Ä–∞—Å–∏–≤—É—é HTML –∫–∞—Ä—Ç–æ—á–∫—É
    st.markdown(f"""
    <div class="winner-box">
        <div class="winner-icon" style="font-size:35px; margin-bottom:10px;">{emoji_icon}</div>
        <div class="winner-title" style="color: #aaa; font-size: 13px; font-weight: 600; text-transform: uppercase; letter-spacing: 1px;">{title}</div>
        <div class="winner-name">{winner}</div>
        <div class="winner-score" style="color:#555; font-weight:700;">{score_display} {final_suffix}</div>
    </div>
    """, unsafe_allow_html=True)
    
    # –ö–Ω–æ–ø–∫–∞ –¥–ª—è –æ—Ç–∫—Ä—ã—Ç–∏—è –º–æ–¥–∞–ª—å–Ω–æ–≥–æ –æ–∫–Ω–∞
    if st.button("üîç –ü–æ–¥—Ä–æ–±–Ω–µ–µ", key=f"btn_{title}",width='stretch'):
        show_winner_details(title, description, sorted_items, suffix)


# ---------------- –í–ò–ö–¢–û–†–ò–ù–ê –° –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–ï–ô –ó–ê–ì–†–£–ó–ö–ò ----------------
@fragment
def render_quiz_tab(df, selected):
    st.subheader("üéÆ –£–≥–∞–¥–∞–π –∞–≤—Ç–æ—Ä–∞")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–µ—Å—Å–∏–∏ –¥–ª—è –≤–∏–∫—Ç–æ—Ä–∏–Ω—ã
    if 'quiz_state' not in st.session_state:
        st.session_state.quiz_state = "intro"
        st.session_state.quiz_score = 0
        st.session_state.quiz_index = 0
        st.session_state.quiz_questions = []
        st.session_state.quiz_last_res = None

    # –°–û–°–¢–û–Ø–ù–ò–ï 1: –≠–ö–†–ê–ù –ü–†–ò–í–ï–¢–°–¢–í–ò–Ø
    if st.session_state.quiz_state == "intro":
        st.markdown("""<div style="text-align:center; padding: 20px;"><h3>–ü–æ–ø—Ä–æ–±—É–π —É–≥–∞–¥–∞—Ç—å, –∫—Ç–æ —ç—Ç–æ –Ω–∞–ø–∏—Å–∞–ª!</h3></div>""", unsafe_allow_html=True)
        
        # –ö–Ω–æ–ø–∫–∞ —Å—Ç–∞—Ä—Ç–∞
        if st.button("üöÄ –ù–ê–ß–ê–¢–¨ –ò–ì–†–£", width='stretch'):
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é, —á—Ç–æ –º—ã —Ä–∞–±–æ—Ç–∞–µ–º
            with st.spinner('üé≤ –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º –º–∏–ª–ª–∏–æ–Ω —Å–æ–æ–±—â–µ–Ω–∏–π... –∏—â–µ–º –ª—É—á—à–∏–µ –≤–æ–ø—Ä–æ—Å—ã...'):
                start_search = time.time() # –¢–∞–π–º–µ—Ä –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                
                # 1. –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è (—Å–∞–º–∞—è —Ç—è–∂–µ–ª–∞—è —á–∞—Å—Ç—å)
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –∫–æ–ª–æ–Ω–∫–∞-–æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä (–∏–∑ –ø—Ä–æ—à–ª–æ–≥–æ —à–∞–≥–∞)
                if "is_quiz_candidate" in df.columns:
                    # –ë—ã—Å—Ç—Ä–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –±—É–ª–µ–≤–æ–π –º–∞—Å–∫–µ
                    quiz_pool = df[
                        (df["is_quiz_candidate"] == True) & 
                        (df["from"].isin(selected))
                    ]
                else:
                    # –†–µ–∑–µ—Ä–≤–Ω—ã–π –º–µ–¥–ª–µ–Ω–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç (–µ—Å–ª–∏ optimization –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∞)
                    st.warning("‚ö†Ô∏è –†–∞–±–æ—Ç–∞–µ–º –≤ –º–µ–¥–ª–µ–Ω–Ω–æ–º —Ä–µ–∂–∏–º–µ (–Ω–µ—Ç –∏–Ω–¥–µ–∫—Å–∞)")
                    quiz_pool = df[
                        (df["text"].str.len() > 30) & 
                        (df["text"].str.len() < 250) & 
                        (df["from"].isin(selected))
                    ]
                
                # –û—Ç–ª–∞–¥–∫–∞: —Å–∫–æ–ª—å–∫–æ –Ω–∞—à–ª–∏ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
                # st.toast(f"–ù–∞–π–¥–µ–Ω–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤: {len(quiz_pool)}") 

                if len(quiz_pool) < 10:
                    st.error(f"–°–ª–∏—à–∫–æ–º –º–∞–ª–æ —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –∏–≥—Ä—ã ({len(quiz_pool)}). –í—ã–±–µ—Ä–∏—Ç–µ –±–æ–ª—å—à–µ –∞–≤—Ç–æ—Ä–æ–≤!")
                else:
                    # 2. –í—ã–±–æ—Ä–∫–∞ 10 —Å–ª—É—á–∞–π–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤
                    try:
                        subset = quiz_pool.sample(10)
                        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π (—Å–∞–º—ã–π –±—ã—Å—Ç—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã)
                        st.session_state.quiz_questions = subset[['text', 'from', 'date']].to_dict('records')
                        
                        # 3. –°–º–µ–Ω–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è
                        st.session_state.quiz_state = "playing"
                        st.session_state.quiz_score = 0
                        st.session_state.quiz_index = 0
                        st.session_state.quiz_last_res = None
                        
                        # –ó–∞–º–µ—Ä –≤—Ä–µ–º–µ–Ω–∏ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                        # st.toast(f"–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∑–∞–Ω—è–ª–∞: {time.time() - start_search:.2f} —Å–µ–∫")
                        
                        st.rerun()
                    except Exception as e:
                        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–±–æ—Ä–∫–µ –≤–æ–ø—Ä–æ—Å–æ–≤: {e}")

    # –°–û–°–¢–û–Ø–ù–ò–ï 2: –ò–ì–†–û–í–û–ô –ü–†–û–¶–ï–°–°
    elif st.session_state.quiz_state == "playing":
        q_idx = st.session_state.quiz_index
        
        # –ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å—ã –∫–æ–Ω—á–∏–ª–∏—Å—å
        if q_idx >= 10:
            st.session_state.quiz_state = "finished"
            st.rerun()
            
        q_data = st.session_state.quiz_questions[q_idx]
        
        # –ü—Ä–æ–≥—Ä–µ—Å—Å –±–∞—Ä
        st.progress((q_idx) / 10)
        st.markdown(f"**–í–æ–ø—Ä–æ—Å {q_idx + 1}/10**")
        
        # –°–∞–º–æ —Å–æ–æ–±—â–µ–Ω–∏–µ
        st.markdown(f"""
        <div class="quiz-container" style="background-color: #f0f2f6; padding: 20px; border-radius: 10px; margin-bottom: 20px; text-align: center; font-size: 1.2em;">
            "{q_data['text']}"
        </div>
        """, unsafe_allow_html=True)
        
        # –ö–Ω–æ–ø–∫–∏ –æ—Ç–≤–µ—Ç–æ–≤
        if st.session_state.quiz_last_res is None:
            st.write("–ö—Ç–æ —ç—Ç–æ –Ω–∞–ø–∏—Å–∞–ª?")
            cols = st.columns(len(selected))
            for i, author in enumerate(selected):
                # –ö–ª—é—á buttons –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —à–∞–≥–∞
                if cols[i].button(author, key=f"ans_{q_idx}_{i}", width='stretch'):
                    if author == q_data['from']:
                        st.session_state.quiz_score += 1
                        st.session_state.quiz_last_res = "correct"
                    else:
                        st.session_state.quiz_last_res = "wrong"
                    st.rerun()
        else:
            # –ü–æ–∫–∞–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            if st.session_state.quiz_last_res == "correct":
                st.success(f"‚úÖ –í–ï–†–ù–û! –≠—Ç–æ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ {q_data['from']}")
            else:
                st.error(f"‚ùå –ú–ò–ú–û! –≠—Ç–æ –±—ã–ª(–∞) {q_data['from']}")
                
            if st.button("–°–ª–µ–¥—É—é—â–∏–π –≤–æ–ø—Ä–æ—Å ‚û°Ô∏è", width='stretch', key="next_btn"):
                st.session_state.quiz_index += 1
                st.session_state.quiz_last_res = None
                st.rerun()

    # –°–û–°–¢–û–Ø–ù–ò–ï 3: –§–ò–ù–ê–õ
    elif st.session_state.quiz_state == "finished":
        score = st.session_state.quiz_score
        
        st.markdown(f"""
        <div class="winner-box" style="padding: 40px; text-align: center; background-color: #d4edda; border-radius: 15px; border: 2px solid #c3e6cb;"> 
            <h1 style="color: #155724;">üèÅ –¢–≤–æ–π —Å—á–µ—Ç: {score}/10</h1> 
            <p style="font-size: 1.2em;">–¢—ã –æ—Ç–ª–∏—á–Ω–æ –∑–Ω–∞–µ—à—å –≤–∞—à—É –ø–µ—Ä–µ–ø–∏—Å–∫—É!</p> 
        </div>
        """, unsafe_allow_html=True)
        
        if score > 8:
            st.balloons()
            
        if st.button("üîÑ –°—ã–≥—Ä–∞—Ç—å —Å–Ω–æ–≤–∞", width='stretch'):
            st.session_state.quiz_state = "intro"
            st.rerun()

# ---------------- –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• ----------------
df_raw = load_data()
if df_raw is None:
    st.error("‚ö†Ô∏è –§–∞–π–ª result.json –Ω–µ –Ω–∞–π–¥–µ–Ω! –ü–æ–ª–æ–∂–∏ –µ–≥–æ –≤ –ø–∞–ø–∫—É —Å –ø—Ä–æ–µ–∫—Ç–æ–º.")
    st.stop()
profiler.checkpoint("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö (load_data)")

authors = df_raw["from"].unique().tolist()
with st.sidebar:
    st.header("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏")
    selected = st.multiselect("–£—á–∞—Å—Ç–Ω–∏–∫–∏", authors, default=authors)
    if st.button("üîÑ –û–±–Ω–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ"):
        st.session_state.clear()
        st.cache_data.clear()
        st.rerun()
    
    st.markdown("---")
    st.markdown("### üé≤ –ú–æ–º–µ–Ω—Ç –∏–∑ –∂–∏–∑–Ω–∏")
    if st.button("–ü–æ–∫–∞–∑–∞—Ç—å —Å–ª—É—á–∞–π–Ω–æ–µ"):
        random_msg = df_raw.sample(1).iloc[0]
        st.info(f"**{random_msg['date'].strftime('%d.%m.%Y')}:**\n\n{random_msg['text']}")

df = df_raw[df_raw["from"].isin(selected)].copy()
df["hour"] = df["date"].dt.hour
df["len"] = df["text"].apply(len)

markov_model = build_markov_model(df["text"])

# ---------------- –ì–õ–ê–í–ù–ê–Ø –°–¢–†–ê–ù–ò–¶–ê (HERO HEADER) ----------------
# –£–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ä—ã–π st.title("üíñ 100 –î–Ω–µ–π –í–º–µ—Å—Ç–µ"), —Ç–∞–∫ –∫–∞–∫ —É –Ω–∞—Å —Ç–µ–ø–µ—Ä—å –∫—Ä–∞—Å–∏–≤—ã–π header
start_date = datetime(2025, 9, 13, 22, 35, 0)
now = datetime.now()
diff = now - start_date

days = diff.days
hours = (diff.seconds // 3600)
minutes = (diff.seconds % 3600) // 60

# CSS —Å—Ç–∏–ª–∏ –≤—ã–Ω–µ—Å–µ–Ω—ã –æ—Ç–¥–µ–ª—å–Ω–æ –∏ —ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω—ã, HTML —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω–æ
st.markdown(f"""
<style>
    .hero-container {{
        background: linear-gradient(120deg, #ff9a9e 0%, #fecfef 100%);
        border-radius: 25px;
        padding: 40px 20px;
        text-align: center;
        color: white;
        box-shadow: 0 15px 30px rgba(255, 105, 180, 0.3);
        margin-bottom: 30px;
        position: relative;
        overflow: hidden;
    }}
    .hero-title {{ font-family: 'Pacifico', cursive; font-size: 3em; margin: 0; text-shadow: 2px 2px 4px rgba(0,0,0,0.1); }}
    .hero-days {{ font-size: 5em; font-weight: 800; line-height: 1; margin: 10px 0; }}
    .hero-subtitle {{ font-size: 1.2em; font-weight: 600; opacity: 0.9; }}
    .heart-beat {{ animation: heartbeat 1.5s infinite; display: inline-block; }}
    
    @keyframes heartbeat {{
        0% {{ transform: scale(1); }}
        50% {{ transform: scale(1.2); }}
        100% {{ transform: scale(1); }}
    }}
</style>

<div class="hero-container">
    <div class="hero-title">üéÄ100 –î–Ω–µ–π –í–º–µ—Å—Ç–µüéÄ</div>
    <div class="hero-days">{days}<span style="font-size:0.3em; margin-left:10px;">–¥–Ω–µ–π</span></div>
    <div class="hero-subtitle">
        {hours} —á. {minutes} –º–∏–Ω. <span class="heart-beat">‚ù§Ô∏è</span> –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–π –ª—é–±–≤–∏
    </div>
</div>
""", unsafe_allow_html=True)
# ---------------- –ö–≠–®–ò–†–û–í–ê–ù–ò–ï –ì–õ–û–ë–ê–õ–¨–ù–û–ô –°–¢–ê–¢–ò–°–¢–ò–ö–ò ----------------
@st.cache_data(show_spinner="–ü–æ–¥—Å—á–µ—Ç —Å–ª–æ–≤ –∏ —Å–∏–º–≤–æ–ª–æ–≤ (1 –º–ª–Ω —Å–æ–æ–±—â–µ–Ω–∏–π)...")
def get_global_metrics(df):
    """
    –°—á–∏—Ç–∞–µ—Ç –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É: —Å–ª–æ–≤–∞, —Å–∏–º–≤–æ–ª—ã, —Ç–æ–ø—ã –æ—Ç–ø—Ä–∞–≤–∏—Ç–µ–ª–µ–π.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—é –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏.
    """
    # 1. –°—á–∏—Ç–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ (–±—ã—Å—Ç—Ä—ã–π –º–µ—Ç–æ–¥ —á–µ—Ä–µ–∑ –ø–æ–¥—Å—á–µ—Ç –ø—Ä–æ–±–µ–ª–æ–≤)
    # –≠—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ 50 —Ä–∞–∑ –±—ã—Å—Ç—Ä–µ–µ, —á–µ–º split() –∫–∞–∂–¥–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è
    if "text" in df.columns:
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Å—Ç—Ä–æ–∫–∏ –∏ —Å—á–∏—Ç–∞–µ–º –ø—Ä–æ–±–µ–ª—ã + 1 = –ø—Ä–∏–º–µ—Ä–Ω–æ–µ –∫–æ–ª-–≤–æ —Å–ª–æ–≤
        # fillna('') –Ω—É–∂–Ω–æ, —á—Ç–æ–±—ã –Ω–µ —É–ø–∞–ª–æ –Ω–∞ –ø—É—Å—Ç—ã—Ö
        text_series = df["text"].fillna("").astype(str)
        word_counts = text_series.str.count(' ') + 1
        total_words = word_counts.sum()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∞–≤—Ç–æ—Ä–∞–º (–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ from)
        # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π DF –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏, —á—Ç–æ–±—ã –Ω–µ –∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å –≤–µ—Å—å –æ–≥—Ä–æ–º–Ω—ã–π df
        temp_df = pd.DataFrame({
            'from': df['from'],
            'words': word_counts,
            'chars': df['len'] # –ú—ã –ø–æ—Å—á–∏—Ç–∞–ª–∏ len –µ—â–µ –≤ load_data
        })
        
        author_stats = temp_df.groupby('from').sum()
    else:
        total_words = 0
        author_stats = pd.DataFrame()

    # 2. –û–±—â–∏–µ —Ü–∏—Ñ—Ä—ã
    total_msg = len(df)
    total_days = (df["date"].max() - df["date"].min()).days if not df.empty else 0
    profiler.checkpoint("–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≥–ª–æ–±–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
    return total_msg, total_words, total_days, author_stats
# ---------------- –ö–≠–®–ò–†–û–í–ê–ù–ò–ï –¢–Ø–ñ–ï–õ–´–• –ì–†–ê–§–ò–ö–û–í (–û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø) ----------------
@st.cache_data(show_spinner="–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–±–ª–∞–∫–∞ —Å–ª–æ–≤...")
def get_heavy_analytics(df):
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ–±–ª–∞–∫–æ –∏ N-–≥—Ä–∞–º–º—ã.
    –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ê–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è N-–≥—Ä–∞–º–º (20k –≤–º–µ—Å—Ç–æ 1M).
    """
    wc = None
    ngrams_list = []
    
    if "text" in df.columns:
        # 1. –û–ë–õ–ê–ö–û –°–õ–û–í
        # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –¥–ª–∏–Ω–Ω—ã–µ —Å–ª–æ–≤–∞, —á—Ç–æ–±—ã —É—Å–∫–æ—Ä–∏—Ç—å —Å–∫–ª–µ–π–∫—É
        # –ï—Å–ª–∏ —Å—Ç—Ä–æ–∫ > 200k, –±–µ—Ä–µ–º —Å—ç–º–ø–ª –¥–ª—è –æ–±–ª–∞–∫–∞ —Ç–æ–∂–µ (–≤–∏–∑—É–∞–ª—å–Ω–æ —Ä–∞–∑–Ω–∏—Ü—ã –Ω–µ—Ç)
        if len(df) > 200000:
            text_data = df["text"].dropna().sample(200000).astype(str)
        else:
            text_data = df["text"].dropna().astype(str)
            
        text_combined = " ".join([t for t in text_data if len(t) > 3])
        text_clean = re.sub(r'[^–∞-—è—ëa-z\s]', '', text_combined.lower())
        
        if text_clean:
            try:
                mask = np.array(Image.open("heart_mask.png")) if os.path.exists("heart_mask.png") else None
                # Max words –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º, —á—Ç–æ–±—ã –±—ã—Å—Ç—Ä–µ–µ —Ä–µ–Ω–¥–µ—Ä–∏–ª–æ—Å—å
                wc = WordCloud(
                    width=600, height=400, 
                    background_color="white", 
                    colormap="Reds",
                    mask=mask,
                    max_words=100, 
                    stopwords=STOP_WORDS
                ).generate(text_clean)
            except Exception:
                pass

        # 2. N-–ì–†–ê–ú–ú–´ (–¢–£–¢ –ë–´–õ –¢–û–†–ú–û–ó)
        # –ë–µ—Ä–µ–º 20 000 —Å–ª—É—á–∞–π–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π. –≠—Ç–æ–≥–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è –ø–æ–∏—Å–∫–∞ —á–∞—Å—Ç—ã—Ö —Ñ—Ä–∞–∑.
        # –≠—Ç–æ —Å–Ω–∏–∑–∏—Ç –≤—Ä–µ–º—è —Å 5 —Å–µ–∫ –¥–æ 0.3 —Å–µ–∫.
        sample_size = min(20000, len(df))
        sample_text_series = df["text"].dropna().sample(sample_size).astype(str)
        sample_text = " ".join(sample_text_series)
        
        # –ë—ã—Å—Ç—Ä–∞—è –æ—á–∏—Å—Ç–∫–∞
        words = re.sub(r'[^–∞-—è—ëa-z\s]', '', sample_text.lower()).split()
        words = [w for w in words if w not in STOP_WORDS and len(w) > 2]
        
        if words:
            bi_grams = zip(words, words[1:])
            counts = Counter(bi_grams)
            for bigram, count in counts.most_common(10):
                # –§–∏–ª—å—Ç—Ä—É–µ–º –º—É—Å–æ—Ä
                if count > 1:
                    ngrams_list.append((f"{bigram[0]} {bigram[1]}", count))

    return wc, ngrams_list
# ---------------- –ö–≠–®–ò–†–û–í–ê–ù–ò–ï –ó–ê–õ–ê –°–õ–ê–í–´ (–° –ü–û–î–î–ï–†–ñ–ö–û–ô –î–ê–¢–´) ----------------
@st.cache_data(show_spinner="–ü–æ–¥—Å—á–µ—Ç –ø—Ä–æ—Ñ–∏–ª–µ–π –ª–∏—á–Ω–æ—Å—Ç–∏ –∏ —Å–ª–æ–≤–∞—Ä—è...")
def get_hall_of_fame_data(df, selected_authors, start_date=None):
    """
    –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏.
    start_date: –¥–∞—Ç–∞ –Ω–∞—á–∞–ª–∞ –æ—Ç–Ω–æ—à–µ–Ω–∏–π. –ï—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω–∞, –º–µ—Ç—Ä–∏–∫–∏ –Ω–µ–∂–Ω–æ—Å—Ç–∏ —Å—á–∏—Ç–∞—é—Ç—Å—è –æ—Ç –Ω–µ—ë.
    """
    if "hour" not in df.columns:
        df["hour"] = df["date"].dt.hour

    # --- –ë–õ–û–ö 1: –û–ë–©–ê–Ø –ò–°–¢–û–†–ò–Ø (–°—á–∏—Ç–∞–µ–º –ø–æ –≤—Å–µ–º—É –≤—Ä–µ–º–µ–Ω–∏) ---
    
    # 1. –ì–õ–ê–í–ù–´–ô –ë–û–õ–¢–£–ù
    msg_counts = df["from"].value_counts().to_dict()

    # 2. –°–ê–ú–´–ô –ë–´–°–¢–†–´–ô
    df_sorted = df.sort_values("date")
    time_diffs = df_sorted["date"].diff().dt.total_seconds() / 60
    author_changed = df_sorted["from"] != df_sorted["from"].shift()
    
    replies_df = pd.DataFrame({'from': df_sorted['from'], 'diff': time_diffs})
    replies_df = replies_df[author_changed & (replies_df['diff'] < 720)]
    reply_speed = replies_df.groupby("from")['diff'].mean().to_dict()

    # 3. –õ–ï–í –¢–û–õ–°–¢–û–ô
    len_mean = df.groupby("from")["len"].mean().to_dict()

    # 4. –ò–ù–ò–¶–ò–ê–¢–û–†
    initiators_mask = time_diffs > 360
    initiators = df_sorted[initiators_mask]["from"].value_counts().to_dict()

    # 5. –ü–û–ß–ï–ú–£–ß–ö–ê
    questions_count = df[df["text"].str.contains(r"\?", na=False)]["from"].value_counts().to_dict()



    # 7. –ú–ï–î–ò–ê –ò –°–°–´–õ–ö–ò
    links_count = df[df["text"].str.contains("http", na=False)]["from"].value_counts().to_dict()
    
    if "media_type" in df.columns:
        media_count = df[df["media_type"].notna() | (df["media_type"] != "")] ["from"].value_counts().to_dict()
    elif "file" in df.columns:
        media_count = df[df["file"].notna()]["from"].value_counts().to_dict()
    else:
        media_count = {}

    # 8. –ñ–ê–í–û–†–û–ù–û–ö –ò –°–û–í–ê
    lark_count = df[(df["hour"] >= 6) & (df["hour"] <= 10)]["from"].value_counts().to_dict()
    owl_count = df[(df["hour"] >= 0) & (df["hour"] <= 4)]["from"].value_counts().to_dict()

    # --- –ë–õ–û–ö 2: –ü–ï–†–ò–û–î –û–¢–ù–û–®–ï–ù–ò–ô (–ù–µ–∂–Ω–æ—Å—Ç—å, –≠–º–æ–¥–∑–∏, –ò–∑–≤–∏–Ω–µ–Ω–∏—è) ---
    # –ï—Å–ª–∏ –¥–∞—Ç–∞ –∑–∞–¥–∞–Ω–∞, —Ñ–∏–ª—å—Ç—Ä—É–µ–º df. –ï—Å–ª–∏ –Ω–µ—Ç - –±–µ—Ä–µ–º –≤–µ—Å—å.
    if start_date:
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º start_date –≤ datetime64 –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        ts_start = pd.to_datetime(start_date)
        df_period = df[df['date'] >= ts_start]
    else:
        df_period = df
        
    # –°—á–∏—Ç–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–æ–±—â–µ–Ω–∏–π –ó–ê –ü–ï–†–ò–û–î (–¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤)
    msg_counts_period = df_period["from"].value_counts().to_dict()
    print(msg_counts_period)
    # 9. –ú–ò–õ–ê–®–ö–ê (–°–ª–æ–≤–∞ –ª—é–±–≤–∏)
    cute_mask = df_period["text"].str.contains(r"–∫—Ä–∞—Å–∏–≤|–ª—é–±–∏–º|–ª—É—á—à|—Å–æ–ª–Ω|—É–º–Ω|–º–∏–ª|—Ä–æ–¥–Ω|–∑–∞–π|–∫–æ—Ç|–ø—Ä–µ–∫—Ä–∞—Å|–æ–±–æ–∂–∞—é|–Ω–µ–∂–Ω|—Å–∫—É—á–∞|–ª—é–±–ª|–∞—Ö—É–µ–Ω|—Ü–µ–ª—É|–º—É–∞|—Å–µ–∫—Å|–ø—Ä–∏–Ω—Ü|—Å–ª–∞–¥|–∑–æ–ª–æ—Ç|–ª—É—á—à|—è –±–æ–ª|—Å–ª–∞–¥–∫|—Ö–æ—á—É —Ç–µ–±—è|–ª—É—á—à|–º—É–≤", case=False, na=False)
    cute_count = df_period[cute_mask]["from"].value_counts().to_dict()

    # 10. –≠–ú–û–î–ó–ò (–ü–æ –ø–µ—Ä–∏–æ–¥—É)
    emoji_counts = {}
    # 11. –°–õ–û–í–ê–†–ù–´–ô –ó–ê–ü–ê–° (–û—Å—Ç–∞–≤–ª—è–µ–º –ø–æ –≤—Å–µ–π –∏—Å—Ç–æ—Ä–∏–∏, —ç—Ç–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç)
    vocab_counts = {}
    # 12. –ò–ó–í–ò–ù–ï–ù–ò–Ø (–ü–æ –ø–µ—Ä–∏–æ–¥—É)
    apology_mask = df_period["text"].str.contains(r"–ø—Ä–æ—Å—Ç–∏|–∏–∑–≤–∏–Ω–∏|sorry|–≤–∏–Ω–æ–≤–∞—Ç|—Å—Ç—ã–¥", case=False, na=False)
    apology_count = df_period[apology_mask]["from"].value_counts().to_dict()

    # 13. --- –ù–û–í–ê–Ø –ú–ï–¢–†–ò–ö–ê: –ü–û–î–î–ï–†–ñ–ö–ê (The Therapist) ---
    support_regex = r"–≤—Å—ë –±—É–¥–µ—Ç|–ø–µ—Ä–µ–∂–∏–≤–∞|—Å–ø—Ä–∞–≤–∏|—Å–ø–æ–∫–æ–π|–∑–∞–±–µ–π|–Ω–æ—Ä–º|–¥–µ—Ä–∂–∏—Å—å|–ø–æ–Ω–∏–º–∞—é|–Ω–µ –±–æ–π—Å—è|–≤—Å—ë —Ö–æ—Ä–æ—à–æ|–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é|–∫–∞–∫ —Ç—ã"
    support_mask = df_period["text"].str.contains(support_regex, case=False, na=False)
    support_count = df_period[support_mask]["from"].value_counts().to_dict()
    
    #6. –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å
    hype_regex = r"–æ–≥–æ|–≤–∞—É|–∫—Ä—É—Ç–æ|–∂–µ—Å—Ç—å|—Å—É–ø–µ—Ä|–∫–ª–∞—Å—Å|–æ—Ñ–∏–≥–µ—Ç—å|—à–æ–∫|!!|–∫–∞–ø–µ—Ü|—É–∂–∞—Å|–ø–∏–∑–¥–µ—Ü|–∞—Ö—É–µ—Ç—å"
    hype_mask = df_period["text"].str.contains(hype_regex, case=False, na=False)
    hype_count = df_period[hype_mask]["from"].value_counts().to_dict()

    try:
        for author in selected_authors:
            # –°–ª–æ–≤–∞—Ä—å —Å—á–∏—Ç–∞–µ–º –ø–æ –≤—Å–µ–π –∏—Å—Ç–æ—Ä–∏–∏ (df)
            full_text = " ".join(df[df["from"] == author]["text"].dropna().tolist())
            vocab_counts[author] = len(set(full_text.split()))

            # –≠–º–æ–¥–∑–∏ —Å—á–∏—Ç–∞–µ–º –ø–æ –ø–µ—Ä–∏–æ–¥—É (df_period)
            period_text = df_period[df_period["from"] == author]["text"].dropna()
            if 'extract_emojis' in globals():
                emoji_counts[author] = period_text.apply(lambda x: len(extract_emojis(x))).sum()
            else:
                emoji_counts[author] = 0

    except:
        emoji_counts = {a: 0 for a in selected_authors}
        vocab_counts = {a: 0 for a in selected_authors}

    return {
        # –û–±—â–∏–µ (–ó–∞–ª –°–ª–∞–≤—ã)
        "msg_counts": msg_counts,
        "reply_speed": reply_speed,
        "len_mean": len_mean,
        "initiators": initiators,
        "questions": questions_count,
        "hype": hype_count,           # –ó–ê–ú–ï–ù–ê (–±—ã–ª–æ laughter)
        "support": support_count,     # –ù–û–í–û–ï
        "media": media_count,
        "links": links_count,
        "lark": lark_count,
        "owl": owl_count,
        
        # –ü–µ—Ä–∏–æ–¥ (–ù–µ–∂–Ω–æ—Å—Ç—å)
        "msg_counts_period": msg_counts_period, 
        "cute": cute_count,
        "emoji": emoji_counts,
        "apology": apology_count,
        "hype_period": hype_count,       # –ó–ê–ú–ï–ù–ê (–±—ã–ª–æ laughter_period)
        "support_period": support_count, # –ù–û–í–û–ï
        
        # –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç
        "vocab": vocab_counts
    }
# ---------------- –ö–≠–®–ò–†–û–í–ê–ù–ò–ï –ì–†–ê–§–ò–ö–û–í (–í–ï–†–°–ò–Ø –° –¢–ê–ô–ú–õ–ê–ô–ù–û–ú) ----------------
@st.cache_data(show_spinner="–ê–Ω–∞–ª–∏–∑ –∏—Å—Ç–æ—Ä–∏–∏, —Å—Ç–∏–∫–µ—Ä–æ–≤ –∏ –≤–∞–∂–Ω—ã—Ö —Å–æ–±—ã—Ç–∏–π...")
def get_charts_data(df):
    """
    –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤.
    –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –°–æ–∑–¥–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å—Ç–∏–∫–µ—Ä–æ–≤ —á–µ—Ä–µ–∑ shift(), —É–±–∏—Ä–∞—è —Ü–∏–∫–ª—ã –ø–æ–∏—Å–∫–∞.
    """
    # 1. –î–ê–ù–ù–´–ï –î–õ–Ø –ò–°–¢–û–†–ò–ò
    daily_counts = df.set_index('date').resample('D').size().reset_index(name='count')
    
    # 2. –î–ê–ù–ù–´–ï –î–õ–Ø –ë–ê–õ–ê–ù–°–ê
    user_counts = df['from'].value_counts()
    
    # 3. –î–ê–ù–ù–´–ï –î–õ–Ø –°–¢–ò–ö–ï–†–û–í
    stickers_df = pd.DataFrame()
    sticker_contexts = {} # –ù–æ–≤—ã–π –æ–±—ä–µ–∫—Ç –¥–ª—è –≥–æ—Ç–æ–≤—ã—Ö —Å–ª–æ–≤–∞—Ä–µ–π
    if "file" in df.columns:
        # –í–ï–ö–¢–û–†–ò–ó–ê–¶–ò–Ø –ö–û–ù–¢–ï–ö–°–¢–ê (–°–∞–º–æ–µ –≤–∞–∂–Ω–æ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ)
        # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –∫–æ–ª–æ–Ω–∫—É —Å —Ç–µ–∫—Å—Ç–æ–º –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è (—Å–¥–≤–∏–≥ –≤–Ω–∏–∑ –Ω–∞ 1)
        # –≠—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤ —è–¥—Ä–µ C++, –ø–æ—ç—Ç–æ–º—É –º–≥–Ω–æ–≤–µ–Ω–Ω–æ, –≤ –æ—Ç–ª–∏—á–∏–µ –æ—Ç Python-—Ü–∏–∫–ª–æ–≤
        df_context = df.copy() # –†–∞–±–æ—Ç–∞–µ–º —Å –∫–æ–ø–∏–µ–π, —á—Ç–æ–±—ã –Ω–µ –ª–æ–º–∞—Ç—å –æ—Å–Ω–æ–≤–Ω–æ–π df
        df_context['prev_text'] = df_context['text'].shift(1)
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å—Ç–∏–∫–µ—Ä—ã
        mask = (df_context["file"].str.contains(r'\.webp|\.tgs', na=False)) | \
               (df_context["type"] == "sticker") | \
               (df_context["media_type"] == "sticker")
        
        # –ë–µ—Ä–µ–º —Å—Ç–∏–∫–µ—Ä—ã —Å—Ä–∞–∑—É —Å –ø—Ä–∏–∫–ª–µ–µ–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        stickers_df = df_context[mask].copy()
        
        # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω–æ–µ
        cols_needed = ['file', 'from', 'date', 'prev_text'] # prev_text —É–∂–µ —Ç—É—Ç!
        stickers_df = stickers_df[[c for c in cols_needed if c in stickers_df.columns]]
        # --- –ì–õ–ê–í–ù–ê–Ø –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø (–ü–†–ï-–í–´–ß–ò–°–õ–ï–ù–ò–ï) ---
        # –ú—ã –∑–∞—Ä–∞–Ω–µ–µ —Å–æ–±–∏—Ä–∞–µ–º —Å–ø–∏—Å–∫–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ –≤ —Å–ª–æ–≤–∞—Ä–∏.
        # –≠—Ç–æ —Ç—è–∂–µ–ª–∞—è –æ–ø–µ—Ä–∞—Ü–∏—è, –Ω–æ —Ç–µ–ø–µ—Ä—å –æ–Ω–∞ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è 1 —Ä–∞–∑ –≤ –∫—ç—à–µ.
        
        # 1. –°–ª–æ–≤–∞—Ä—å –¥–ª—è —Ä–µ–∂–∏–º–∞ "–í—Å–µ –≤–º–µ—Å—Ç–µ": Key=File -> Value=[Contexts]
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º dropna(), —á—Ç–æ–±—ã –Ω–µ —Ö—Ä–∞–Ω–∏—Ç—å –º—É—Å–æ—Ä
        dict_all = stickers_df.groupby('file')['prev_text'].apply(lambda x: x.dropna().tolist()).to_dict()
        
        # 2. –°–ª–æ–≤–∞—Ä—å –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –∞–≤—Ç–æ—Ä–æ–≤: Key=(File, Author) -> Value=[Contexts]
        dict_auth = stickers_df.groupby(['file', 'from'])['prev_text'].apply(lambda x: x.dropna().tolist()).to_dict()
        
        sticker_contexts = {
            "all": dict_all,
            "auth": dict_auth
        }
    # 4. –î–ê–ù–ù–´–ï –î–õ–Ø –î–ù–ï–ô –ù–ï–î–ï–õ–ò
    day_counts = df["date"].dt.day_name().value_counts()

    # 5. –î–ê–ù–ù–´–ï –î–õ–Ø –¢–ï–ü–õ–û–í–û–ô –ö–ê–†–¢–´
    days_mapped = df["date"].dt.day_name().map(
        {'Monday': '–ü–Ω', 'Tuesday': '–í—Ç', 'Wednesday': '–°—Ä', 'Thursday': '–ß—Ç', 
         'Friday': '–ü—Ç', 'Saturday': '–°–±', 'Sunday': '–í—Å'}
    )
    hours = df["date"].dt.hour
    hm_source = pd.DataFrame({'day_name_ru': days_mapped, 'hour': hours})
    hm_data = hm_source.groupby(["day_name_ru", "hour"]).size().reset_index(name='count')

    # 6. –î–ê–ù–ù–´–ï –î–õ–Ø –¢–ê–ô–ú–õ–ê–ô–ù–ê
    events = []
    
    # –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –≤–Ω—É—Ç—Ä–∏ –∫—ç—à–∞
    def check_event(mask, title, icon):
        try:
            # –ò—â–µ–º –ø–µ—Ä–≤–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
            # head(1) –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É—Å–∫–æ—Ä—è–µ—Ç —Ä–∞–±–æ—Ç—É –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø–æ–ª–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π
            matches = df[mask].head(1) 
            if not matches.empty:
                first = matches.iloc[0]
                events.append({
                    "date": first['date'],
                    "title": title,
                    "text": first['text'],
                    "author": first['from'],
                    "icon": icon
                })
        except:
            pass

    # --- –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –°–û–ë–´–¢–ò–ô (–õ–û–ì–ò–ö–ê –ò–ó –¢–ê–ë–ê) ---
    # 1. –ì–õ–ê–í–ù–´–ï –°–õ–û–í–ê
    if "text" in df.columns:
        check_event(df['text'].str.contains("–ª—é–±–ª—é —Ç–µ–±—è", case=False, na=False), "–ü–µ—Ä–≤–æ–µ '–õ—é–±–ª—é —Ç–µ–±—è'", "‚ù§Ô∏è")
        check_event(df['text'].str.contains("–æ–±–æ–∂–∞—é", case=False, na=False), "–ü–µ—Ä–≤–æ–µ '–û–±–æ–∂–∞—é'", "ü•∞")
        check_event(df['text'].str.contains("—Å–∫—É—á–∞—é", case=False, na=False), "–ü–µ—Ä–≤–æ–µ '–°–∫—É—á–∞—é'", "ü•∫")

        # 2. –ú–ò–õ–´–ï –ü–†–û–ó–í–ò–©–ê
        check_event(df['text'].str.contains("–°–æ–ª–Ω—Ü–µ", case=False, na=False), "–ü–µ—Ä–≤–æ–µ '–°–æ–ª–Ω—ã—à–∫–æ'", "‚òÄÔ∏è")
        check_event(df['text'].str.contains("–ª–∏—Å", case=False, na=False), "–ü–µ—Ä–≤—ã–π '–õ–∏—Å'", "üê±")
        check_event(df['text'].str.contains("–ú–∏–ª—ã–π", case=False, na=False), "–ü–µ—Ä–≤—ã–π '–ú–∏–ª—ã–π'", "üê∞")
        check_event(df['text'].str.contains("–ø—Ä–∏–Ω—Ü–µ—Å—Å–∞", case=False, na=False), "–ü–µ—Ä–≤–æ–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ —Ç–∏—Ç—É–ª–∞", "üëë")
        check_event(df['text'].str.contains("–∫—Ä–∞—Å–∏–≤|–ø—Ä–µ–∫—Ä–∞—Å", case=False, na=False), "–ü–µ—Ä–≤—ã–π –∫–æ–º–ø–ª–∏–º–µ–Ω—Ç", "üòç")

        # 3. –î–ï–ô–°–¢–í–ò–Ø –ò –í–°–¢–†–ï–ß–ò
        check_event(df['text'].str.contains("–º–æ–∂–Ω–æ –∏ –ø–æ–≥—É–ª—è—Ç—å", case=False, na=False), "–ü–µ—Ä–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –≤—Å—Ç—Ä–µ—á–∏", "üåπ")
        check_event(df['text'].str.contains("—Ñ–∏–ª—å–º|—Å–µ—Ä–∏–∞–ª", case=False, na=False), "–ü–µ—Ä–≤–æ–µ –æ–±—Å—É–∂–¥–µ–Ω–∏–µ –∫–∏–Ω–æ", "üé¨")
        check_event(df['text'].str.contains("–∫—É—à–∞—Ç—å", case=False, na=False), "–ü–µ—Ä–≤—ã–π —Ä–∞–∑–≥–æ–≤–æ—Ä –æ –µ–¥–µ", "üçï")
        check_event(df['text'].str.contains("—Å–ø–∞—Ç|—Å–æ–Ω|–∫—Ä–æ–≤–∞—Ç—å", case=False, na=False), "–ü–µ—Ä–≤–æ–µ '–ü–æ—Ä–∞ —Å–ø–∞—Ç—å'", "üò¥")
        check_event(df['text'].str.contains("–≥—É–ª—è—Ç—å|–ø—Ä–æ–≥—É–ª–∫–∞", case=False, na=False), "–ü–µ—Ä–≤–∞—è –ø—Ä–æ–≥—É–ª–∫–∞", "üå≥")

        # 4. –≠–ú–û–¶–ò–ò –ò –†–ò–¢–£–ê–õ–´
        check_event(df['text'].str.contains("–¥–æ–±—Ä–æ–µ —É—Ç—Ä–æ", case=False, na=False), "–ü–µ—Ä–≤–æ–µ '–î–æ–±—Ä–æ–µ —É—Ç—Ä–æ'", "‚òï")
        check_event(df['text'].str.contains("—Å–ø–æ–∫–æ–π–Ω–æ–π –Ω–æ—á–∏|—Å–ª–∞–¥–∫–∏—Ö —Å–Ω–æ–≤", case=False, na=False), "–ü–µ—Ä–≤–∞—è '–°–ø–æ–∫–æ–π–Ω–æ–π –Ω–æ—á–∏'", "üåô")
        check_event(df['text'].str.contains("–ø—Ä–æ—Å—Ç–∏|–∏–∑–≤–∏–Ω–∏", case=False, na=False), "–ü–µ—Ä–≤–æ–µ –∏–∑–≤–∏–Ω–µ–Ω–∏–µ", "ü§ù")
        check_event(df['text'].str.contains("–∞—Ö–∞—Ö|–ª–æ–ª|–æ—Ä—É|rfl", case=False, na=False), "–ü–µ—Ä–≤—ã–π —Å–º–µ—Ö", "üòÇ")
        check_event(df['text'].str.contains("–æ–±–µ—â–∞—é", case=False, na=False), "–ü–µ—Ä–≤–æ–µ –æ–±–µ—â–∞–Ω–∏–µ", "ü§û")
        check_event(df['text'].str.contains("—Å–ø–∞—Å–∏–±–æ|–±–ª–∞–≥–æ–¥–∞—Ä—é", case=False, na=False), "–ü–µ—Ä–≤–∞—è –±–ª–∞–≥–æ–¥–∞—Ä–Ω–æ—Å—Ç—å", "üôè")
        check_event(df['text'].str.contains("–¥–∞–π—Å–æ–Ω", case=False, na=False), "–ü–µ—Ä–≤—ã–π '–î–∞–π—Å–æ–Ω'", "üòè")

        # 5. –¢–ï–•–ù–ò–ß–ï–°–ö–û–ï
        check_event(df['text'].str.contains("http", case=False, na=False), "–ü–µ—Ä–≤–∞—è —Å—Å—ã–ª–∫–∞", "üîó")

    # –ú–µ–¥–∏–∞ —Å–æ–±—ã—Ç–∏—è
    if "media_type" in df.columns:
        check_event(df['media_type'] == 'sticker', "–ü–µ—Ä–≤—ã–π —Å—Ç–∏–∫–µ—Ä", "üé≠")
        check_event(df['media_type'].isin(['photo', 'video_file']), "–ü–µ—Ä–≤–æ–µ —Ñ–æ—Ç–æ/–≤–∏–¥–µ–æ", "üì∏")
        check_event(df['media_type'] == 'voice_message', "–ü–µ—Ä–≤–æ–µ –≥–æ–ª–æ—Å–æ–≤–æ–µ", "üé§")
        check_event(df['media_type'] == 'video_message', "–ü–µ—Ä–≤—ã–π –∫—Ä—É–∂–æ—á–µ–∫", "üîµ")

    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏
    events.sort(key=lambda x: x.get('date', pd.Timestamp.min))
    profiler.checkpoint("–¢—è–∂—ë–ª—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω—ã")
    # –í–û–ó–í–†–ê–©–ê–ï–ú 7 –≠–õ–ï–ú–ï–ù–¢–û–í (sticker_contexts –Ω–∞ 6-–º –º–µ—Å—Ç–µ)
    return daily_counts, user_counts, stickers_df, day_counts, hm_data, sticker_contexts, events
# –ü—Ä–æ–≥—Ä–µ—Å—Å –±–∞—Ä (–æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –±—ã–ª–æ, –ø—Ä–æ—Å—Ç–æ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –º–µ—Å—Ç–∞ –≤—Å—Ç–∞–≤–∫–∏)
col_prog1, col_prog2 = st.columns([4, 1])
with col_prog1:
    st.caption(f"üöÄ –ü—É—Ç—å –∫ –≥–æ–¥—É (–æ—Å—Ç–∞–ª–æ—Å—å {365 - diff.days} –¥–Ω.)")
    progress = min(max(diff.days / 365, 0.0), 1.0)
    st.progress(progress)
with col_prog2:
    st.caption(f"**{int(progress*100)}%**")
profiler.checkpoint("–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
st.markdown("---")

tabs = st.tabs([
    "üèÜ –ó–∞–ª –°–ª–∞–≤—ã", 
    "üìà –ê–∫—Ç–∏–≤–Ω–æ—Å—Ç—å", 
    "‚öñÔ∏è –ë–∞–ª–∞–Ω—Å & –ù–µ–∂–Ω–æ—Å—Ç—å",
    "üîÆ –í–∞–Ω–≥–∞",
    "‚è≥ –ò—Å—Ç–æ—Ä–∏—è (–ü–µ—Ä–≤—ã–µ)",
    "üîé –ü–æ–∏—Å–∫", 
    "üé≠ –°—Ç–∏–∫–µ—Ä—ã",
    "‚òÅÔ∏è –°–ª–æ–≤–∞",
    "üéÆ –í–∏–∫—Ç–æ—Ä–∏–Ω–∞"
])
profiler.checkpoint("–°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–æ–≤")
# ================== –¢–ê–ë 1: –ì–õ–ê–í–ù–ê–Ø (–û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø) ==================
with tabs[0]:
    # 1. –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ (–±–µ—Ä–µ–º –∏–∑ get_global_metrics, –æ–Ω–∞ —É–∂–µ —É –Ω–∞—Å –µ—Å—Ç—å)
    total_msg, total_words, total_days, author_stats = get_global_metrics(df)
    
    st.subheader("üìä –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞")
    c1, c2, c3, c4 = st.columns(4)
    c1.metric("üíå –í—Å–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏–π", f"{total_msg:,}".replace(",", " "))
    c2.metric("üìÖ –î–Ω–µ–π –≤–º–µ—Å—Ç–µ", total_days)
    
    # –û–±—â–∏–π –≤–µ—Å —Å–∏–º–≤–æ–ª–æ–≤ (–±—ã—Å—Ç—Ä–æ —á–µ—Ä–µ–∑ sum)
    total_chars = df['len'].sum() if "len" in df.columns else 0
    c3.metric("üìù –¢—ã—Å—è—á —Å–∏–º–≤–æ–ª–æ–≤", f"{total_chars/1000:.1f}k")
    
    # –°–ª–æ–≤–æ "–ª—é–±–ª—é" (–±—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫ –±–µ–∑ regex –∏–ª–∏ —Å simple regex)
    love_count = df["text"].str.contains("–ª—é–±–ª—é", case=False, na=False).sum()
    c4.metric("‚ù§Ô∏è –°–ª–æ–≤ '–õ—é–±–ª—é'", love_count)

    st.markdown("### üèÜ –ù–∞—à –ó–∞–ª –°–ª–∞–≤—ã")
    st.caption("–ù–∞–∂–º–∏—Ç–µ –Ω–∞ –∫–Ω–æ–ø–∫—É –ø–æ–¥ –∫–∞—Ä—Ç–æ—á–∫–æ–π, —á—Ç–æ–±—ã —É–∑–Ω–∞—Ç—å –¥–µ—Ç–∞–ª–∏")

    # 2. –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∫–∞—Ä—Ç–æ—á–µ–∫ –∏–∑ –ö–≠–®–ê
    # –≠—Ç–æ —Å–∞–º–∞—è –≤–∞–∂–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ - –æ–Ω–∞ –∑–∞–º–µ–Ω—è–µ—Ç 10 —Å–µ–∫—É–Ω–¥ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –Ω–∞ 0.01 —Å–µ–∫
    hof = get_hall_of_fame_data(df, selected)

    # –†—è–¥ 1
    r1c1, r1c2, r1c3, r1c4 = st.columns(4)
    with r1c1: draw_winner_card("–ì–ª–∞–≤–Ω—ã–π –±–æ–ª—Ç—É–Ω", hof["msg_counts"], "ü¶ú", description="–£ –∫–æ–≥–æ –±–æ–ª—å—à–µ –≤—Å–µ–≥–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π.")
    with r1c2: draw_winner_card("–°–∞–º—ã–π –±—ã—Å—Ç—Ä—ã–π", hof["reply_speed"], "üöÄ", "–º–∏–Ω", reverse=True, description="–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ —Å–æ–æ–±—â–µ–Ω–∏–µ (—á–µ–º –º–µ–Ω—å—à–µ, —Ç–µ–º –ª—É—á—à–µ).")
    with r1c3: draw_winner_card("–õ–µ–≤ –¢–æ–ª—Å—Ç–æ–π", hof["len_mean"], "‚úçÔ∏è", "—Å–∏–º–≤.", description="–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –æ–¥–Ω–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è –≤ —Å–∏–º–≤–æ–ª–∞—Ö.")
    with r1c4: draw_winner_card("–ò–Ω–∏—Ü–∏–∞—Ç–æ—Ä", hof["initiators"], "üí°", description="–ö—Ç–æ —á–∞—â–µ –ø–∏—à–µ—Ç –ø–µ—Ä–≤—ã–º –ø–æ—Å–ª–µ –ø–µ—Ä–µ—Ä—ã–≤–∞ –≤ –æ–±—â–µ–Ω–∏–∏ (> 6 —á–∞—Å–æ–≤).")
    
    st.write("") 
    
    # –†—è–¥ 2
    r2c1, r2c2, r2c3, r2c4 = st.columns(4)
    with r2c1: draw_winner_card("–ü–æ—á–µ–º—É—á–∫–∞", hof["questions"], "ü§î", description="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–æ–±—â–µ–Ω–∏–π —Å –≤–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω—ã–º –∑–Ω–∞–∫–æ–º.")
    with r2c2: draw_winner_card("–†–µ–∞–∫—Ü–∏–æ–Ω–µ—Ä", hof["hype"], "üî•", description="–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä —ç–Ω–µ—Ä–≥–∏–∏: '–û–ì–û!', '–ñ–ï–°–¢–¨', '–ö–†–£–¢–û'.")
    with r2c3: draw_winner_card("–ú–µ–¥–∏–∞-–º–∞–≥–Ω–∞—Ç", hof["media"], "üé¨", description="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö —Ñ–æ—Ç–æ, –≤–∏–¥–µ–æ –∏ –≥–æ–ª–æ—Å–æ–≤—ã—Ö.")
    with r2c4: draw_winner_card("–ö–æ—Ä–æ–ª—å —Å—Å—ã–ª–æ–∫", hof["links"], "üåê", description="–°–∫–æ–ª—å–∫–æ —Å—Å—ã–ª–æ–∫ (http...) –±—ã–ª–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ.")

    st.write("")

    # –†—è–¥ 3
    r3c1, r3c2, r3c3, r3c4 = st.columns(4)
    with r3c1: draw_winner_card("–ñ–∞–≤–æ—Ä–æ–Ω–æ–∫", hof["lark"], "‚òïÔ∏è", description="–°–æ–æ–±—â–µ–Ω–∏—è, –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ —É—Ç—Ä–æ–º (—Å 6:00 –¥–æ 10:00).")
    with r3c2: draw_winner_card("–°–æ–≤–∞", hof["owl"], "üåô", description="–°–æ–æ–±—â–µ–Ω–∏—è, –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ –≥–ª—É–±–æ–∫–æ–π –Ω–æ—á—å—é (—Å 00:00 –¥–æ 04:00).")
    with r3c3: draw_winner_card("–≠–º–æ–¥–∑–∏-–º–∞—Å—Ç–µ—Ä", hof["emoji"], "üòú", description="–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö —Å–º–∞–π–ª–∏–∫–æ–≤.")
    with r3c4: draw_winner_card("–ú–∏–ª–∞—à–∫–∞", hof["cute"], "ü•∞", description="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å–ª–æ–≤ –ª—é–±–≤–∏, –Ω–µ–∂–Ω–æ—Å—Ç–∏ –∏ –∫–æ–º–ø–ª–∏–º–µ–Ω—Ç–æ–≤.")
    
    profiler.checkpoint("–û—Ç—Ä–∏—Å–æ–≤–∫–∞ –≥–ª–∞–≤–Ω–æ–π –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
# ================== –¢–ê–ë 2: –°–¢–ê–¢–ò–°–¢–ò–ö–ê (FIXED) ==================
with tabs[1]:
    # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –î–æ–±–∞–≤–∏–ª–∏ *_, —á—Ç–æ–±—ã —Å–æ–±—Ä–∞—Ç—å –ª–∏—à–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è (timeline_events) –∏ –Ω–µ –≤—ã–∑—ã–≤–∞—Ç—å –æ—à–∏–±–∫—É
    daily, user_counts, _, day_counts, hm_data, *_ = get_charts_data(df)
    
    col_act1, col_act2 = st.columns([2, 1])
    
    # –ì—Ä–∞—Ñ–∏–∫ 1: –î–∏–Ω–∞–º–∏–∫–∞ (Area Chart)
    with col_act1:
        st.subheader("üìà –î–∏–Ω–∞–º–∏–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏–π")
        try:
            fig_daily = px.area(daily, x='date', y='count', 
                                title='–ù–∞—à–∞ –∏—Å—Ç–æ—Ä–∏—è –ø–æ –¥–Ω—è–º', 
                                labels={'date':'–î–∞—Ç–∞', 'count':'–°–æ–æ–±—â–µ–Ω–∏–π'})
            fig_daily.update_traces(line_color='#FF69B4', fill='tozeroy')
            fig_daily.update_layout(margin=dict(l=0, r=0, t=30, b=0))
            st.plotly_chart(fig_daily, use_container_width=True)
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –≥—Ä–∞—Ñ–∏–∫–∞ –¥–∏–Ω–∞–º–∏–∫–∏: {e}")
        
    # –ì—Ä–∞—Ñ–∏–∫ 2: –î–Ω–∏ –Ω–µ–¥–µ–ª–∏ (Bar Chart)
    with col_act2:
        st.subheader("üìÖ –õ—é–±–∏–º—ã–π –¥–µ–Ω—å")
        try:
            fig_bar = px.bar(day_counts, 
                            x=day_counts.index, 
                            y=day_counts.values,
                            color_discrete_sequence=['#FFB6C1'])
            fig_bar.update_layout(showlegend=False, 
                                xaxis_title=None, 
                                yaxis_title=None,
                                margin=dict(l=0, r=0, t=30, b=0))
            st.plotly_chart(fig_bar, use_container_width=True)
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –≥—Ä–∞—Ñ–∏–∫–∞ –¥–Ω–µ–π: {e}")

    # –ì—Ä–∞—Ñ–∏–∫ 3: –¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ (Heatmap)
    st.subheader("üïí –ö–∞—Ä—Ç–∞ –Ω–∞—à–µ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ (–ß–∞—Å—ã)")
    try:
        days_order = ['–ü–Ω', '–í—Ç', '–°—Ä', '–ß—Ç', '–ü—Ç', '–°–±', '–í—Å']
        
        fig_hm = px.density_heatmap(
            hm_data, x="hour", y="day_name_ru", z="count", 
            color_continuous_scale="RdPu",
            labels={"hour": "–ß–∞—Å", "day_name_ru": "–î–µ–Ω—å", "count": "–°–æ–æ–±—â–µ–Ω–∏–π"},
            category_orders={"day_name_ru": days_order},
            title="–ö–æ–≥–¥–∞ –Ω–∞–º –∂–∞—Ä—á–µ –≤—Å–µ–≥–æ –æ–±—â–∞—Ç—å—Å—è? üî•"
        )
        fig_hm.update_layout(xaxis_dtick=1, margin=dict(l=0, r=0, t=30, b=0))
        st.plotly_chart(fig_hm, use_container_width=True)
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ —Ç–µ–ø–ª–æ–≤–æ–π –∫–∞—Ä—Ç—ã: {e}")

    profiler.checkpoint("–û—Ç—Ä–∏—Å–æ–≤–∫–∞ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
# ================== –¢–ê–ë 3: –ë–ê–õ–ê–ù–° & –ù–ï–ñ–ù–û–°–¢–¨ (FIXED DATE) ==================
with tabs[2]: # –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∏–Ω–¥–µ–∫—Å (–æ–±—ã—á–Ω–æ tabs[2] –∏–ª–∏ tabs[3])
    st.subheader("‚öñÔ∏è –ü—Ä–æ—Ñ–∏–ª–∏ –õ–∏—á–Ω–æ—Å—Ç–∏")
    
    # 1. –ò—Å–ø–æ–ª—å–∑—É–µ–º –≥–ª–æ–±–∞–ª—å–Ω—É—é –∫–æ–Ω—Å—Ç–∞–Ω—Ç—É REL_START_DATE
    # –ü–µ—Ä–µ–¥–∞–µ–º –µ—ë –≤ —Ñ—É–Ω–∫—Ü–∏—é. 
    # get_hall_of_fame_data –≤–µ—Ä–Ω–µ—Ç –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤ 'metrics_raw' 
    # –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∑–∞ –ø–µ—Ä–∏–æ–¥ –æ—Ç–Ω–æ—à–µ–Ω–∏–π –≤ –∫–ª—é—á–∞—Ö 'cute', 'apology' –∏ —Ç.–¥.
    hof = get_hall_of_fame_data(df, selected, start_date=REL_START_DATE)
    
    metrics_raw = {}
    for auth in selected:
        # –û–±—â–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ (—Å—á–∏—Ç–∞–µ–º –ø–æ –≤—Å–µ–π –ø–µ—Ä–µ–ø–∏—Å–∫–µ –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø—Ä–æ—Ñ–∏–ª—è)
        msg_count = hof["msg_counts"].get(auth, 0)
        avg_len = hof["len_mean"].get(auth, 0)
        vocab = hof["vocab"].get(auth, 0)
        
        # –≠–º–æ–¥–∑–∏ –∏ –°–∫–æ—Ä–æ—Å—Ç—å —Ç–æ–∂–µ –±–µ—Ä–µ–º –æ–±—â–∏–µ (–∏–ª–∏ –º–æ–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ period, –µ—Å–ª–∏ —Ö–æ—Ç–∏—Ç–µ)
        # –î–ª—è —Ä–∞–¥–∞—Ä–∞ –ª—É—á—à–µ –±—Ä–∞—Ç—å –æ–±—â–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –ø–æ–≤–µ–¥–µ–Ω–∏—è
        e_count = hof["emoji"].get(auth, 0) # –ó–¥–µ—Å—å emoji –≤–µ—Ä–Ω—É—Ç—Å—è –∑–∞ –ø–µ—Ä–∏–æ–¥ (—Å–º. —Ñ—É–Ω–∫—Ü–∏—é), —ç—Ç–æ –æ–∫
        
        # –í–∞–∂–Ω–æ: –¥–µ–ª–∏–º –Ω–∞ –∫–æ–ª-–≤–æ —Å–æ–æ–±—â–µ–Ω–∏–π –ó–ê –ü–ï–†–ò–û–î, –µ—Å–ª–∏ –º–µ—Ç—Ä–∏–∫–∞ –∑–∞ –ø–µ—Ä–∏–æ–¥
        msg_count_period = hof["msg_counts_period"].get(auth, 1)
        if msg_count_period == 0: msg_count_period = 1
        
        emoji_ratio = (e_count / msg_count_period) 
        
        speed_val = hof["reply_speed"].get(auth, 60)
        if pd.isna(speed_val): speed_val = 60
        
        metrics_raw[auth] = {
            "–ë–æ–ª—Ç–ª–∏–≤–æ—Å—Ç—å": msg_count,      # –û–±—â–∞—è
            "–ú–Ω–æ–≥–æ—Å–ª–æ–≤–Ω–æ—Å—Ç—å": avg_len,     # –û–±—â–∞—è
            "–°–ª–æ–≤–∞—Ä–Ω—ã–π –∑–∞–ø–∞—Å": vocab,      # –û–±—â–∏–π
            "–≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å": emoji_ratio,# –ó–∞ –ø–µ—Ä–∏–æ–¥ (—Ç–∞–∫ –∫–∞–∫ emoji —Å—á–∏—Ç–∞–µ–º –ø–æ start_date)
            "–°–∫–æ—Ä–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–∞": speed_val   # –û–±—â–∞—è
        }

    # --- Radar Chart (–û—Ç—Ä–∏—Å–æ–≤–∫–∞) ---
    if metrics_raw:
        categories = ["–ë–æ–ª—Ç–ª–∏–≤–æ—Å—Ç—å", "–ú–Ω–æ–≥–æ—Å–ª–æ–≤–Ω–æ—Å—Ç—å", "–°–ª–æ–≤–∞—Ä–Ω—ã–π –∑–∞–ø–∞—Å", "–≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å", "–°–∫–æ—Ä–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–∞"]
        max_vals = {cat: 0 for cat in categories}
        for auth in metrics_raw:
            for cat in categories:
                if cat != "–°–∫–æ—Ä–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–∞":
                    max_vals[cat] = max(max_vals[cat], metrics_raw[auth][cat])
        
        fig_radar = go.Figure()
        colors = {"–ü—Ä–∏–Ω—Ü": "#636EFA", "–ü—Ä–∏–Ω—Ü–µ—Å—Å–∞": "#FF69B4"} 
        
        for auth in metrics_raw:
            values = []
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
            values.append((metrics_raw[auth]["–ë–æ–ª—Ç–ª–∏–≤–æ—Å—Ç—å"] / max_vals["–ë–æ–ª—Ç–ª–∏–≤–æ—Å—Ç—å"]) * 100 if max_vals["–ë–æ–ª—Ç–ª–∏–≤–æ—Å—Ç—å"] else 0)
            values.append((metrics_raw[auth]["–ú–Ω–æ–≥–æ—Å–ª–æ–≤–Ω–æ—Å—Ç—å"] / max_vals["–ú–Ω–æ–≥–æ—Å–ª–æ–≤–Ω–æ—Å—Ç—å"]) * 100 if max_vals["–ú–Ω–æ–≥–æ—Å–ª–æ–≤–Ω–æ—Å—Ç—å"] else 0)
            values.append((metrics_raw[auth]["–°–ª–æ–≤–∞—Ä–Ω—ã–π –∑–∞–ø–∞—Å"] / max_vals["–°–ª–æ–≤–∞—Ä–Ω—ã–π –∑–∞–ø–∞—Å"]) * 100 if max_vals["–°–ª–æ–≤–∞—Ä–Ω—ã–π –∑–∞–ø–∞—Å"] else 0)
            values.append((metrics_raw[auth]["–≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å"] / max_vals["–≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å"]) * 100 if max_vals["–≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å"] else 0)
            s = metrics_raw[auth]["–°–∫–æ—Ä–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–∞"]
            speed_score = max(0, 100 - s)
            values.append(speed_score)
            values.append(values[0])
            
            fig_radar.add_trace(go.Scatterpolar(
                r=values, theta=categories + [categories[0]], fill='toself', name=auth, line_color=colors.get(auth, None)
            ))

        fig_radar.update_layout(
            polar=dict(radialaxis=dict(visible=True, range=[0, 100])),
            showlegend=True, title="–ö—Ç–æ –≤ —á–µ–º –∫—Ä—É—á–µ?", height=500,
            margin=dict(l=50, r=50, t=50, b=50)
        )
        st.plotly_chart(fig_radar, use_container_width=True)
    
    st.markdown("---")

    # 3. –£—Ä–æ–≤–µ–Ω—å –Ω–µ–∂–Ω–æ—Å—Ç–∏ (–°–¢–†–û–ì–û –û–¢ –î–ê–¢–´ –û–¢–ù–û–®–ï–ù–ò–ô)
    st.subheader("üß∏ –£—Ä–æ–≤–µ–Ω—å –Ω–µ–∂–Ω–æ—Å—Ç–∏")
    # –ö—Ä–∞—Å–∏–≤–æ —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –¥–∞—Ç—É –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–∞
    date_lbl = REL_START_DATE.strftime('%d.%m.%Y')
    st.caption(f"–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å—á–∏—Ç–∞–µ—Ç—Å—è —Å –º–æ–º–µ–Ω—Ç–∞ –Ω–∞—á–∞–ª–∞ –æ—Ç–Ω–æ—à–µ–Ω–∏–π: {date_lbl}")
    
    plot_data = []
    tenderness_scores = {}
    
    for auth in selected:
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏, –∏—Å–ø–æ–ª—å–∑—É—è –Ω–æ–≤—ã–µ –∫–ª—é—á–∏ –∏–∑ return —Ñ—É–Ω–∫—Ü–∏–∏
        # –û–±—Ä–∞—Ç–∏ –≤–Ω–∏–º–∞–Ω–∏–µ: hof["..."] –¥–æ–ª–∂–Ω—ã —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å –∫–ª—é—á–∞–º–∏ –≤ return
        msg_counts_period = hof["msg_counts_period"].get(auth, 0)

        cute_val = hof["cute"].get(auth, 0)
        hype_val = hof["hype_period"].get(auth, 0)      # –≠–º–æ—Ü–∏–∏ (–∑–∞ –ø–µ—Ä–∏–æ–¥)
        support_val = hof["support_period"].get(auth, 0) # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ (–∑–∞ –ø–µ—Ä–∏–æ–¥)
        apology_val = hof["apology"].get(auth, 0)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å—Ç–æ–ª–±—Ü–æ–≤
        plot_data.append({"User": auth, "Type": "–ú–∏–ª–æ—Ç–∞ ü•∞", "Count": cute_val})
        plot_data.append({"User": auth, "Type": "–≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—åü§©", "Count": hype_val})
        plot_data.append({"User": auth, "Type": "–ü–æ–¥–¥–µ—Ä–∂–∫–∞ üíï", "Count": support_val})
        plot_data.append({"User": auth, "Type": "–ò–∑–≤–∏–Ω–µ–Ω–∏—è üôè", "Count": apology_val})
        
        # –†–∞—Å—á–µ—Ç –∏–Ω–¥–µ–∫—Å–∞ (–û—á–∫–∏: –ú–∏–ª–æ—Ç–∞ + –ü–æ–¥–¥–µ—Ä–∂–∫–∞*1.5 + –≠–º–æ—Ü–∏–∏*0.5 - –ò–∑–≤–∏–Ω–µ–Ω–∏—è*0.5)
        score = (cute_val + (support_val * 3) + (hype_val * 0.2) - (apology_val * 0.2))/(msg_counts_period/100)
        tenderness_scores[auth] = score

    plot_df = pd.DataFrame(plot_data)
    
    c_bal1, c_bal2 = st.columns([2, 1])
    with c_bal1:
        if not plot_df.empty:
            fig_bal = px.bar(plot_df, x="User", y="Count", color="Type", barmode="group",
                                color_discrete_map={"–ú–∏–ª–æ—Ç–∞ ü•∞": "#FF69B4", "–≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—åü§©": "#FFD700", "–ò–∑–≤–∏–Ω–µ–Ω–∏—è üôè": "#A9A9A9","–ü–æ–¥–¥–µ—Ä–∂–∫–∞ üíï": "#FF00E6"},
                                title="–û —á–µ–º –º—ã –≥–æ–≤–æ—Ä–∏–º —á–∞—â–µ?")
            fig_bal.update_layout(plot_bgcolor='rgba(0,0,0,0)', margin=dict(l=0, r=0, t=30, b=0))
            st.plotly_chart(fig_bal, use_container_width=True)
        
    with c_bal2:
        st.markdown("### üå°Ô∏è –ò–Ω–¥–µ–∫—Å –õ—é–±–≤–∏")
        st.caption("–ü—Ä–æ—Ü–µ–Ω—Ç —Å–æ–æ–±—â–µ–Ω–∏–π —Å –ª–∞—Å–∫–æ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏")
        for auth, score in tenderness_scores.items():
            st.metric(f"{auth}", f"{score:.1f}%", delta="–°—É–ø–µ—Ä!" if score > 3 else "–ù–æ—Ä–º")
            
    profiler.checkpoint("–û—Ç—Ä–∏—Å–æ–≤–∫–∞ –±–∞–ª–∞–Ω—Å–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
# ================== –¢–ê–ë 4: –í–ê–ù–ì–ê ==================
with tabs[3]:
    st.subheader("üîÆ –ù–µ–π—Ä–æ—Å–µ—Ç—å –æ—Ç–Ω–æ—à–µ–Ω–∏–π")
    st.markdown("–í–≤–µ–¥–∏ —Å–ª–æ–≤–æ, –∞ —è –ø–æ–ø—Ä–æ–±—É—é –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å —Ñ—Ä–∞–∑—É —Ç–∞–∫, –∫–∞–∫ —ç—Ç–æ —Å–¥–µ–ª–∞–ª–∏ –±—ã –º—ã:")
    
    col_pred1, col_pred2 = st.columns([1, 2])
    with col_pred1:
        seed = st.text_input("–ù–∞—á–∞–ª–æ —Ñ—Ä–∞–∑—ã:", value="–õ—é–±–ª—é")
        length = st.slider("–°–∫–æ–ª—å–∫–æ —Å–ª–æ–≤ –¥–æ–±–∞–≤–∏—Ç—å?", 3, 20, 8)
        do_predict = st.button("‚ú® –ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å",width='stretch')
    with col_pred2:
        if do_predict and seed:
            last_word = seed.split()[-1]
            prediction = predict_phrase(markov_model, last_word, length)
            full = seed.rsplit(' ', 1)[0] + " " + prediction if len(seed.split()) > 1 else prediction
            st.markdown(f"""<div class="prediction-box">‚ú® {full}...</div>""", unsafe_allow_html=True)
profiler.checkpoint("–û—Ç—Ä–∏—Å–æ–≤–∫–∞ –≤–∞–Ω–≥–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
# ================== –¢–ê–ë 4: –ò–°–¢–û–†–ò–Ø (TIMELINE) [FIXED] ==================
with tabs[4]:
    st.subheader("üìú –ù–∞—à–∞ –•—Ä–æ–Ω–æ–ª–æ–≥–∏—è")
    
    # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –†–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º –≤—Å–µ 7 –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö, —á—Ç–æ–±—ã events –ø–æ–ø–∞–ª –∫—É–¥–∞ –Ω–∞–¥–æ
    # –ü–æ—Ä—è–¥–æ–∫ –≤ get_charts_data: daily, user, stickers, days, hm, contexts, events
    daily, user_counts, stickers_df, day_counts, hm_data, sticker_contexts, events = get_charts_data(df)
    
    if not events:
        st.info("–ò—Å—Ç–æ—Ä–∏—è –ø–æ–∫–∞ –ø—É—Å—Ç–∞. –ü–æ–ø—Ä–æ–±—É–π –≤—ã–±—Ä–∞—Ç—å –±–æ–ª—å—à–µ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ –∏–ª–∏ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Ñ–∞–π–ª –¥–∞–Ω–Ω—ã—Ö.")
    else:
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è HTML
        timeline_html = '<div class="timeline-container">'
        
        for evt in events:
            # –¢–µ–ø–µ—Ä—å evt ‚Äî —ç—Ç–æ —Å–ª–æ–≤–∞—Ä—å, –∏ evt['date'] —Å—Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ
            try:
                date_str = evt['date'].strftime('%d %B %Y')
                time_str = evt['date'].strftime('%H:%M')
                
                # –≠–∫—Ä–∞–Ω–∏—Ä—É–µ–º –∫–∞–≤—ã—á–∫–∏ –∏ –æ–±—Ä–µ–∑–∞–µ–º —Ç–µ–∫—Å—Ç
                clean_msg = str(evt['text']).replace('"', '&quot;')
                if len(clean_msg) > 100: clean_msg = clean_msg[:100] + "..."
                if len(clean_msg) < 2: clean_msg = "<i>(–í–ª–æ–∂–µ–Ω–∏–µ)</i>"
                
                timeline_html += f"""
    <div class="timeline-item">
    <div class="timeline-dot"></div>
    <span class="timeline-date">{date_str} <span style="font-weight:400; opacity:0.7">–≤ {time_str}</span></span>
    <div class="timeline-card">
    <div class="timeline-icon">{evt['icon']}</div>
    <div class="timeline-content">
    <div class="timeline-title">{evt['title']}</div>
    <div class="timeline-text">"{clean_msg}"</div>
    <div class="timeline-author">‚Äî {evt['author']}</div>
    </div>
    </div>
    </div>"""
            except Exception as e:
                # –ù–∞ —Å–ª—É—á–∞–π —Å–±–æ—è –≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º —Å–æ–±—ã—Ç–∏–∏, —á—Ç–æ–±—ã –Ω–µ –ª–æ–º–∞—Ç—å –≤–µ—Å—å —Ç–∞–±
                continue
            
        timeline_html += '</div>'
        st.markdown(timeline_html, unsafe_allow_html=True)
    profiler.checkpoint("–û—Ç—Ä–∏—Å–æ–≤–∫–∞ –∏—Å—Ç–æ—Ä–∏–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
# ================== –¢–ê–ë 6: –ü–û–ò–°–ö ==================
# ================== –¢–ê–ë 6 (–∏–ª–∏ 5): –ü–û–ò–°–ö ==================
with tabs[5]:
    st.subheader("üîé –ü–æ–∏—Å–∫ –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏–π")
    search_query = st.text_input("–ß—Ç–æ –∏—â–µ–º?", placeholder="–ù–∞–ø—Ä–∏–º–µ—Ä: –ª—é–±–ª—é, –º–æ—Ä–µ, –ø–∏—Ü—Ü–∞")
    
    if search_query:
        # –ü–æ–∏—Å–∫ –ø–æ —Ç–µ–∫—Å—Ç—É
        results = df[df["text"].str.contains(search_query, case=False, na=False)]
        
        st.success(f"–ù–∞–π–¥–µ–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–π: **{len(results)}**")
        
        if len(results) > 0:
            # –ì—Ä–∞—Ñ–∏–∫ —á–∞—Å—Ç–æ—Ç—ã —É–ø–æ–º–∏–Ω–∞–Ω–∏–π
            res_daily = results.groupby(results["date"].dt.date).size().reset_index(name='count')
            fig_search = px.bar(res_daily, x='date', y='count', color_discrete_sequence=['#FF69B4'])
            st.plotly_chart(fig_search, width='stretch')
            
            st.markdown("##### –ü–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–∞—Ö–æ–¥–∫–∏:")
            
            # --- –ò–ó–ú–ï–ù–ï–ù–ò–ï: –ü–ï–†–ï–í–û–†–ê–ß–ò–í–ê–ï–ú –†–ï–ó–£–õ–¨–¢–ê–¢–´ ---
            # .iloc[::-1] —Ä–∞–∑–≤–æ—Ä–∞—á–∏–≤–∞–µ—Ç DataFrame –∑–∞–¥–æ–º –Ω–∞–ø–µ—Ä–µ–¥ (—Å–Ω–∞—á–∞–ª–∞ –Ω–æ–≤—ã–µ)
            newest_results = results.iloc[::-1]
            
            # –í—ã–≤–æ–¥–∏–º –ø–µ—Ä–≤—ã–µ 5 –∏–∑ –ü–ï–†–ï–í–ï–†–ù–£–¢–û–ì–û —Å–ø–∏—Å–∫–∞
            for i in range(min(20, len(newest_results))):
                msg = newest_results.iloc[i]
                # –î–æ–±–∞–≤–∏–ª –≥–æ–¥ –≤ –¥–∞—Ç—É, —á—Ç–æ–±—ã –±—ã–ª–æ –ø–æ–Ω—è—Ç–Ω–µ–µ, –∫–æ–≥–¥–∞ —ç—Ç–æ –±—ã–ª–æ
                st.markdown(f"**{msg['date'].strftime('%d.%m.%Y')} {msg['from']}:** {msg['text']}")
profiler.checkpoint("–û—Ç—Ä–∏—Å–æ–≤–∫–∞ –ø–æ–∏—Å–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
# ================== –¢–ê–ë 7: –°–¢–ò–ö–ï–†–´ (GROUPBY –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø) ==================
# ================== –¢–ê–ë 7: –°–¢–ò–ö–ï–†–´ (INSTANT RENDER) ==================
with tabs[6]:
    st.subheader("üé≠ –õ—é–±–∏–º—ã–µ —Å—Ç–∏–∫–µ—Ä—ã")
    
    # –†–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ (–æ–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ sticker_contexts)
    daily, user_counts, stickers_df_raw, day_counts, hm_data, sticker_contexts, *rest = get_charts_data(df)

    if not stickers_df_raw.empty:
        # --- –§–ò–õ–¨–¢–† ---
        f_col1, f_col2 = st.columns([1, 3])
        with f_col1:
            filter_options = ["–í—Å–µ –≤–º–µ—Å—Ç–µ"] + user_counts.index.tolist()
            sticker_author = st.radio("–ß—å–∏ —Å—Ç–∏–∫–µ—Ä—ã —Å–º–æ—Ç—Ä–∏–º?", filter_options, index=0)

        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ç–æ–ª—å–∫–æ –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ —Ç–æ–ø–∞ (—ç—Ç–æ –±—ã—Å—Ç—Ä–æ)
        if sticker_author != "–í—Å–µ –≤–º–µ—Å—Ç–µ":
            st_df = stickers_df_raw[stickers_df_raw["from"] == sticker_author]
        else:
            st_df = stickers_df_raw

        popular_files = st_df["file"].value_counts()
        # –¢–†–ï–ë–û–í–ê–ù–ò–ï: > 10 —Ä–∞–∑
        popular_files = popular_files[popular_files > 10] 
        
        if popular_files.empty:
            st.info(f"–ù–µ—Ç —Å—Ç–∏–∫–µ—Ä–æ–≤ —Å —á–∞—Å—Ç–æ—Ç–æ–π > 10.")
        else:
            cols = st.columns(3)
            
            for idx, (file_path, count) in enumerate(popular_files.items()):
                col = cols[idx % 3]
                with col:
                    with st.container(border=True):
                        try:
                            # –ú–µ–¥–∏–∞
                            if os.path.exists(file_path):
                                if file_path.endswith(".webm"):
                                    st.video(file_path, autoplay=True, loop=True, muted=True, start_time=0)
                                else:
                                    st.image(file_path)
                            else:
                                if os.path.exists(os.path.basename(file_path)):
                                    st.image(os.path.basename(file_path))
                                else:
                                    st.markdown("üñºÔ∏è *—Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω*")
                        except: pass
                        
                        rank_emoji = "ü•á " if idx==0 else "ü•à " if idx==1 else "ü•â " if idx==2 else ""
                        st.markdown(f"<h4 style='text-align:center; color: #FF69B4; margin:5px;'>{rank_emoji}{count}</h4>", unsafe_allow_html=True)
                        
                        # --- –ö–û–ù–¢–ï–ö–°–¢ (–ú–ì–ù–û–í–ï–ù–ù–´–ô) ---
                        # –ë–µ—Ä–µ–º –≥–æ—Ç–æ–≤—ã–π —Å–ø–∏—Å–æ–∫ –∏–∑ –∫—ç—à–∞
                        raw_contexts = []
                        
                        if sticker_author == "–í—Å–µ –≤–º–µ—Å—Ç–µ":
                            # –ë–µ—Ä–µ–º –∏–∑ —Å–ª–æ–≤–∞—Ä—è 'all'
                            raw_contexts = sticker_contexts.get("all", {}).get(file_path, [])
                        else:
                            # –ë–µ—Ä–µ–º –∏–∑ —Å–ª–æ–≤–∞—Ä—è 'auth' –ø–æ –∫–ª—é—á—É (–§–∞–π–ª, –ê–≤—Ç–æ—Ä)
                            raw_contexts = sticker_contexts.get("auth", {}).get((file_path, sticker_author), [])
                        
                        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π (–±—ã—Å—Ç—Ä–∞—è –æ–ø–µ—Ä–∞—Ü–∏—è –≤ –ø–∞–º—è—Ç–∏)
                        valid_contexts = [str(c) for c in raw_contexts if len(str(c)) > 2]

                        if valid_contexts:
                            st.markdown("<div style='font-size:0.8em; color:gray; margin-top:5px;'>–û–±—ã—á–Ω–æ –≤ –æ—Ç–≤–µ—Ç –Ω–∞:</div>", unsafe_allow_html=True)
                            
                            # –¢–†–ï–ë–û–í–ê–ù–ò–ï: –¢–æ–ø-5 –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤
                            most_common = Counter(valid_contexts).most_common(5)
                            
                            for ctx, freq in most_common:
                                clean_ctx = ctx
                                st.markdown(f"""
                                <div style='background:#f0f2f6; padding:4px; border-radius:4px; font-size:0.8em; margin-bottom:2px; border-left: 3px solid #FFB6C1;'>
                                    üì© {clean_ctx} <span style='color:#aaa;'>({freq})</span>
                                </div>
                                """, unsafe_allow_html=True)
    else:
        st.warning("–°—Ç–∏–∫–µ—Ä—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
        
    profiler.checkpoint("–û—Ç—Ä–∏—Å–æ–≤–∫–∞ —Å—Ç–∏–∫–µ—Ä–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")

# ================== –¢–ê–ë 8: –°–õ–û–í–ê ==================
with tabs[7]:
    Create_word_Cloud()
    profiler.checkpoint("–û—Ç—Ä–∏—Å–æ–≤–∫–∞ –æ–±–ª–∞–∫–∞ —Å–ª–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
    st.markdown("#### ü•à –ß–∞—Å—Ç—ã–µ —Ñ—Ä–∞–∑—ã")
    ngrams = get_ngrams(df["text"], 2)
    profiler.checkpoint("–û—Ç—Ä–∏—Å–æ–≤–∫–∞ –Ω–≥—Ä–∞–º –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
    cols_ng = st.columns(5)
    for i, (phrase, count) in enumerate(ngrams[:5]):
        cols_ng[i].metric(phrase.capitalize(), count)

# ================== –¢–ê–ë 9: –í–ò–ö–¢–û–†–ò–ù–ê ==================
with tabs[8]:
    render_quiz_tab(df, selected)
profiler.checkpoint("–û—Ç—Ä–∏—Å–æ–≤–∫–∞ –≤–∫–ª–∞–¥–æ–∫ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
# –§—É—Ç–µ—Ä
st.markdown("---")
st.markdown("<div style='text-align: center; color: #aaa; font-size: 14px;'>–°–æ–∑–¥–∞–Ω–æ —Å ‚ù§Ô∏è –Ω–∞–≤—Å–µ–≥–¥–∞</div>", unsafe_allow_html=True)
profiler.finish()